---
title: "Rapport final"
author: "Paul GUILLOTTE & Jules CORBEL"
date: "01/02/2019"
fontsize : 11pt
output: 
  pdf_document:
    toc: yes
    number_sections: yes 
    fig_caption: yes
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
---
```{r setup, include=FALSE}
require(tseries)
require(knitr)
require(forecast)
require(corrplot)
require(fUnitRoots)
require(vars)

#Code permettant la mise en place de la représentation visuelle de la matrice des corrélations
cor.mtest <- function(mat, conf.level = 0.95){
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
  diag(p.mat) <- 0
  diag(lowCI.mat) <- diag(uppCI.mat) <- 1
  for(i in 1:(n-1)){
    for(j in (i+1):n){
      tmp <- cor.test(mat[,i], mat[,j], conf.level = conf.level)
      p.mat[i,j] <- p.mat[j,i] <- tmp$p.value
      lowCI.mat[i,j] <- lowCI.mat[j,i] <- tmp$conf.int[1]
      uppCI.mat[i,j] <- uppCI.mat[j,i] <- tmp$conf.int[2]
    }
  }
  return(list(p.mat, lowCI.mat, uppCI.mat))
}

EQM<-function(serie, prediction){
  return(sum((serie - prediction)^2)/length(serie))
}

#trim <- read.csv("~/PFE_Time_Series/Data/Data_Trim.csv", sep=";", dec=",")
trim <- read.csv("~/Cygwin/app/home/Jules/PFE_Time_Series/Data/Data_Trim.csv", sep=";", dec=",")
```

#Introduction {-}
Dans le cadre de la formation Génie Informatique et Statistique (GIS), l’un des modules qui nous est proposé est la réalisation d’un projet de fin d’études (PFE) en lien avec une entreprise ou un laboratoire. Notre PFE s’effectue en lien avec l’Institut des Retraites Complémentaires des Employés de Maison (IRCEM) et a pour but de construire des modèles statistiques afin de prédire la masse salariale du secteur d’activité des services à la personne pour les années à venir. 

L’IRCEM a été créée en 1973. Cet organisme à but non lucratif s’occupe de la protection sociale des employés du secteur du service à la personne. Dans ce but, il doit verser des compléments de salaire aux employés à l’aide à la personne. Il effectue alors régulièrement des prévisions de la masse salariale de ce secteur d’activités, afin d’estimer l’argent  qu’il devra verser.

Notre mission pour ce projet est donc de modéliser cette masse salariale et d’effectuer des prévisions pour les années 2018 et 2019, qui seront effectuées en utilisant différentes méthodes de prédiction. La première partie du projet nous a vus nous concentrer sur des méthodes de prédiction univariées, tel que le lissage exponentiel et des modélisations basées sur des processus ARMA. Nous avions également commencé à modéliser la masse salariale à l'aide des variables auxiliaires. Ici, nous présentons nos travaux sur les modèles vectoriels (modèles VAR plus des essais sur les modèles VARMA). L’intégralité du projet s’est effectuée sur le logiciel R.

\newpage
#Description des jeux de données

Les données que nous a fourni l’IRCEM sont comprises dans deux jeux de données représentant deux ensembles de variables distincts : l’un contient des variables annuelles et l’autre des variables trimestrielles, dans les deux cas à partir de 1990. Afin d’aider à la prédiction des valeurs de la masse salariale, nous devrons nous appuyer sur plusieurs variables auxiliaires. Pour les données annuelles, nous disposons de 4 variables : le SMIC horaire brut, le PIB, le taux de chômage et le montant de l’Allocation pour la Garde des Enfants à Domicile. La masse salariale annuelle est connue jusqu’en 2017, et on possède les informations sur les autres variables jusqu’à 2019. En ce qui concerne le modèle trimestriel, nous disposons de 3 variables : le SMIC, le PIB et le taux de chômage des femmes. La masse salariale trimestrielle est connue jusqu’au 2e trimestre de 2017. Le PIB trimestriel est lui connu jusqu’au 1er trimestre 2017. Pour les deux autres variables, les informations que nous possédons vont jusqu’au dernier trimestre de 2017. Le faible nombre de données (surtout pour les variables annuelles) pourra cependant être un obstacle. En effet, dans le jeu annuel, il n’y a 28 années, ce qui représente peu de valeurs pour créer des modèles pertinents. 

\newpage
#Analyse descriptive des séries

##Rappel sur la stationnarité du second ordre

Avant de commencer à analyser les séries, nous rappelons des bases sur des notions dont nous aurons besoin par la suite.

Dans de nombreux modèles de séries temporelles, la série en entrée doit satisfaire une hypothèse de stationnarité. Les conditions de la stationnarité du second ordre sont les suivantes :

$E[y_{t}]=\mu \forall t=1...T$

$Var[y_{t}]=\sigma ^{2}\neq \infty \forall t=1...t$

$Cov[y_{i},Z_{i-k}]=f(k) \forall i=1...t, \forall k=1...t$

Nous nous intéressons dans ce rapport aux différentes séries trimestrielles à notre disposition. Dans un premier temps, nous nous intéressons aux corrélations entre les variables deux à deux afin de nous faire une première idée du lien qu'il existe entre les variables.

##Masse salariale \label{MSE}

```{r, fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig1} Evolution trimestrielle de la masse salariale", echo=F}
  MSE <- ts(trim$MSE, start = 1990, end = c(2017, 2), frequency=4)
  plot(MSE, xaxt="n", cex.main=0.9)
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", 
                                             "2010Q1", "2015Q1"))
```

La masse salariale trimestrielle, représentée en Figure \ref{fig1} possède une composante de tendance de 1990 à 2010. La série tend par la suite à stagner. Nous remarquons également une saisonnalité sur cette série, qui est de plus en plus marquée à mesure que le temps passe. 

```{r, fig.height=3, fig.cap="\\label{fig2} Fonctions d'autocorrélation de la masse salariale trimestrielle", echo=F}
  par(mfrow=c(1,2), cex.main=0.8)
  acf(MSE, main="Auto-corrélation de la
      masse salariale trimestrielle", lag.max=20)
  pacf(MSE, main="Autocorrélation partielle
       de la masse salariale trimestrielle", lag.max=20)
  kpss.test(MSE)
  adf.test(MSE)
```

Comme la série comporte une tendance et une saisonnalité, elle ne correspond pas aux deux premières conditions de la stationnarité du second ordre, soit que la série possède une moyenne et un écart-type constants. Cela est confirmé par la Figure \ref{fig2}, qui nous montre fonction ACF qui décroît régulièrement. Nous effectuons également un test de KPSS (test de stationnarité) servant à vérifier si la série est stationnaire ou non (sous l'hypothèse $H_{0}$ la série est stationnaire, et sous l'hypothèse $H_{1}$ elle ne l'est pas). La série est dite stationnaire si ses propriétés statistiques (espérance, variance et auto-corrélation) sont fixes au cours du temps. La p-value est de 0.01 ce qui nous confirme que la série n'est pas stationnaire avec un risque de première espèce de 5%. Nous mettons également en place un test de racines unitaires, le test de Dickey Fuller augmenté. Son hypothèse nulle est que la série a été générée par un processus présentant une racine unitaire, et donc que la série n'est pas stationnaire. Ici, avec un risque de première espèce à 5%, on conserve l'hypothèse nulle et on conclut, à l'aide des deux tests effectués, que la série n'est pas stationnaire. 

##PIB  \label{PIB}

La Figure \ref{fig3} nous montre l'évolution trimestriel du PIB qui, comme pour la masse salariale possède une tendance. Cependant, elle ne semble pas posséder de saisonnalité. Cette série ne semble donc pas non plus stationnaire. Nous effectuons à nouveau un test de KPSS. La p-value est de 0.01 ce qui nous confirme que la série n'est pas stationnaire avec un risque de première espèce de 5%. Même conclusion au regard du test augmenté de Dickey Fuller.

```{r, fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig3} Evolution trimestrielle du PIB", echo=F}
  PIB <- ts(trim$PIB, start = 1990, end = c(2017, 1), frequency=4)
  plot(PIB, xaxt="n", cex.main=0.9)
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
```

```{r, fig.height=3, fig.cap="\\label{fig4} ACF et PACF du PIB trimestriel", echo=F}
  par(mfrow=c(1,2), cex.main=0.8)
  acf(PIB, main="Auto-corrélation 
      du PIB trimestriel", lag.max=40)
  pacf(PIB, main="Autocorrélation partielle
       du PIB trimestriel", lag.max=40)
  par(mfrow=c(1,1))
  kpss.test(PIB)
  adf.test(PIB)
```

##SMIC

Au regard de la Figure \ref{fig5}, on s'aperçoit qu'il y a bien une tendance. Pour la saisonnalité, il est plus difficile de savoir s'il en existe une ou pas, puisque la série semble augmenter seulement à certains temps. Les tests de KPSS et de Dickey Fuller augmenté nous confirment que la série n'est pas stationnaire.
```{r, fig.width=4, fig.height=3.5, fig.align='center', fig.cap="\\label{fig5} Evolution trimestrielle du SMIC", echo=F}
  SMIC <- ts(trim$SMIC, start = c(1990,1), end = c(2017, 4), frequency = 4)
  plot(SMIC, main="", xaxt="n", cex.main=0.9)
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
```

```{r, fig.height=3, fig.cap="\\label{fig6} ACF et PACF du SMIC trimestriel", echo=F}
  par(mfrow=c(1,2), cex.main=0.8)
  acf(SMIC, main="Auto-corrélation du
      SMIC trimestriel", lag.max=20)
  pacf(SMIC, main="Autocorrélation partielle
       du SMIC trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(SMIC)
  adf.test(SMIC)
```

##Taux de chômage des femmes \label{TCHOF}

Pour cette dernière série (Figure \ref{fig7}) qui représente le taux de chômage trimestriel des femmes, il ne semble pas y avoir de saisonnalité. On remarque cependant qu'il y a bien une tendance, au regard de la Figure \ref{fig8}. En regardant la série de plus près, on s'aperçoit que la tendance semble être "par morceaux" : d'abord une hausse de 1990 à 1996, puis elle décroît jusqu'en 2002, avant d'augmenter à nouveau jusqu'en 2007, de chuter jusqu'en 2010. Si la série ne possède pas une tendance uniforme sur toute la durée étudiée, elle semble donc bien posséder une tendance par morceaux. Les tests KPSS et de Dickey Fuller augmenté nous confirment que la série n'est pas stationnaire, avec un risque de première espèce de 5%.
```{r, fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig7} Evolution trimestrielle du taux de chômage des femmes", echo=F}
  TCHOF <- ts(trim$TCHOF, start = c(1990,1), end = c(2017, 4), frequency = 4)
  plot(TCHOF, xaxt="n", cex.main=0.9)
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
```
```{r, fig.height=3, fig.cap="\\label{fig8} ACF et PACF du taux de chômage des femmes trimestriel", echo=F}
  par(mfrow=c(1,2), cex.main=0.8)
  acf(TCHOF, main="Auto-corrélation du taux de
      chômage des femmes trimestriel", lag.max=20)
  pacf(TCHOF, main="Autocorrélation partielle du
       taux de chômage des femmes trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(TCHOF)
  adf.test(TCHOF)
```

##Calcul des corrélations

```{r, fig.width=4, fig.height=3, fig.cap="\\label{fig9} Corrélations entre les variables trimestrielles", echo=F}
corrplot(cor(trim[1:109,-1]), method = "number", type="lower",
         p.mat=cor.mtest(trim[1:109,-1], 0.95)[[1]], insig="pch",
         col=colorRampPalette(c("blue", "light blue", "red"))(50))
corr <- cor.mtest(trim[1:109,-1], 0.95)[[1]]
rownames(corr) <- c("MSE","PIB","SMIC","TCHOF")
colnames(corr) <- c("MSE","PIB","SMIC","TCHOF")
corr
```

Nous affichons la matrice des corrélations des différentes variables en Figure \ref{fig9}. On se rend compte que le taux de chômage des femmes est corrélé négativement avec toutes les autres variables. Les variables PIB, masse salariale et SMIC sont extrêmement liées entre elles. En regardant le tableau des p-values associées au test de Student (H0 : La corrélation entre les deux variables est nulle), on s'aperçoit que toutes les variables prises deux à deux présentes une corrélation.

##Découpage des séries

Pour chacune des séries, nous allons créer un échantillon d'apprentissage, qui nous permettra de construire les différents modèles, ainsi qu'un échantillon de test, qui nous permettra de comparer les prédictions des modèles construits avec des vraies valeurs. L'échantillon d'apprentissage sera composé de toutes les valeurs du premier trimestre 1990 jusqu'au 4e trimestre 2015, tandis que celui de test comprendra toutes les valeurs à partir du 1er trimestre 2016.

```{r, echo=F}
MSETrain <- window(MSE, start=1990, end=c(2015,4))
MSETest <- window(MSE, start=2016, end=c(2017,2))
PIBTrain <- window(PIB, start=1990, end=c(2015,4))
PIBTest <- window(PIB, start=2016, end=c(2017,1))
SMICTrain <- window(SMIC, start=1990, end=c(2015,4))
SMICTest <- window(SMIC, start=2016, end=c(2017,2))
TCHOFTrain <- window(TCHOF, start=1990, end=c(2015,4))
TCHOFTest <- window(TCHOF, start=2016, end=c(2017,2))
```

##Estimation de la valeur manquante du PIB

Contrairement aux autres variables, nous n'avons à notre disposition pour le PIB que les valeurs jusqu'au premier trimestre de 2017. Ceci nous impose de négliger la dernière valeur de toutes les autres séries pour que toutes les variables soient étudiées sur la même période. Pour éviter ce problème, nous décidons d'estimer la variable du PIB pour le 2e trimestre de 2017. Afin de faire cela, nous allons utiliser la valeur estimée par un modèle SARIMA correspondant à la variable PIB, étant donné que c'est celui qui donnait les meilleures prédictions (voir \ref{Annexe1}).

```{r, echo=F}
PredARIMAPIB<- forecast(auto.arima(PIBTrain), h=6)
new.value <- PredARIMAPIB$mean[6]
PIBTest<-ts(c(PredARIMAPIB$mean, new.value), start = 2016, end = c(2017, 2), frequency=4)
```

\newpage
# Modélisation vectorielle

Dans la première partie du projet, nous avons modélisé les différentes séries séparément, à l'aide de modèles de lissage exponentiel ou des processus ARMA, dont les résultats sont présents en \ref{Annexe1}. Nous avons ensuite effectué d'autres modèles ARMA sur la masse salariale en uilisant les autres variables pour l'expliquer. Ces modèles sont présents en \ref{Annexe2}. Cependant, ce type de modélisation ne nous donne pas de résultats plus performants en terme de prédictions.

Pendant la deuxième partie du projet, nous nous sommes donc attachés à utiliser d'autres méthodes de modélisation. Nous nous sommes concentrés sur les modèles de type vectoriels, qui permettent donc de prédire plusieurs séries temporelles simultanément. La plus grande partie de nos travaux portent sur des modèles Vector Auto-Regressive (VAR) plus quelques tests avec l'ajout d'une partie Moving Average(MA) ce qui nous donne des modèles VARMA.

## Définition des modèles

### Ecriture

Un modèle VAR s'écrit sous la forme suivante : 

  $y_t = \sum_{i = 1}^{p} {A_iy_{t-i}} + u_t$
  
$A_i$ représentent les matrices de coefficients du modèle pour un ordre $i$ et $u_t$ une matrice K-dimensionnelle composée des résidus du modèle (indépendants et identiquement distribués). Enfin, $p$ correspond à l'ordre du modèle, qui est en fait le nombre de valeurs du passé prises en compte pour calculer la valeur présente.

### Hypothèses

#### Stabilité du modèle

Pour que le modèle soit valide, nous devons vérifier que l'hypothèse de stabilité est bien respecté. Cette dernière permet d'assurer que les différentes séries du modèle sont stationnaires. Pour vérifier si un processus VAR est stable, nous devons calculer les valeurs propres de la matrice des coefficients suivantes : 

$$A = \begin{bmatrix}
A_1 & A_2 & \cdots & A_{p-1} & A_p \\
I & 0 & \cdots & 0 & 0 \\
0 & I & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & I & 0 
\end{bmatrix}$$

Si les modules des valeurs propres de A sont inférieures à 1, alors le processus VAR est stable.

#### Hypothèses sur les résidus

Pour que le modèle soit valide, certaines conditions sur les résidus doivent également être validées. Il s'agit des suivantes :

* Homoscédasticité
* Normalité
* Absence d'auto-corrélations et de corrélations croisées

## Transformation des séries

Nous allons maintenant transformer les séries pour les rendre stationnaires, afin de pouvoir appliquer les modèles VAR ensuite. Afin de stationnariser les séries, nous utiliserons la fonction *decompose* qui permet de découper la série en trois : la tendance, la saisonnalité et les résidus, afin de pouvoir ensuite travailler avec les résidus. Nous ne stationnariserons que les échantillons d'apprentissage.

### Masse salariale

```{r, fig.width=4.5, fig.height=4, fig.cap="\\label{fig10} Décomposition de la masse salariale", echo=F}
par(cex.main=0.8)
  plot(decompose(MSETrain, "multiplicative"))
  MSESta <- na.omit(decompose(MSETrain, "multiplicative")$random)
  MSETrend<-window(decompose(MSETrain, "multiplicative")$trend)
  MSESeasonal<-window(decompose(MSETrain, "multiplicative")$seasonal)
```

Nous nous intéressons aux ACF, PACF (visibles en Figure \ref{fig11}) et test de KPSS afin de vérifier si les résidus obtenus à l'aide de la fonction *decompose* sont stationnaires. Bien que l'ACF et la PACF nous mettent en garde d'une possible non stationnarité de la série, la p-value des tests de KPSS et Dickey Fuller augmenté nous amène à confirmer que notre série est désormais stationnarisée (avec un seuil de confiance à 5% pour les deux tests effectués).

```{r, fig.height=2.5, fig.cap="\\label{fig11} Fonctions d'autocorrélation de la masse salariale stationnarisée", echo=F}
  par(mfrow=c(1,2), cex.main=0.8)
  acf(MSESta, main="ACF de la masse
      salariale stationnarisée")
  pacf(MSESta, main="PACF de la masse
       salariale stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(MSESta)
  adf.test(MSESta)
  MSETrendTest <- window(forecast(na.omit(MSETrend), h=8)$mean, start=2016)
  MSESeasonalTest<-ts(c(MSESeasonal[103:104], rep(MSESeasonal[1:4],3)), start=c(2015,3),frequency=4)
```
```{r, fig.width=3.5, fig.height=3, fig.cap="\\label{fig12} Masse salariale trimestrielle stationnarisée", echo=F}
  plot(MSESta, xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
```

\newpage
### PIB

```{r, fig.width=5, fig.height=4.5, fig.cap="\\label{fig13} PIB trimestriel stationnarisé", echo=F}
  PIBSta <- na.omit(decompose(PIBTrain, "multiplicative")$random)
  plot(PIBSta, main="PIB trimestriel stationnarisé", xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
```

```{r, fig.height=2.5, fig.cap="\\label{fig14} Fonctions d'autocorrélation du PIB stationnarisé", echo=F}
  par(mfrow=c(1,2))
  acf(PIBSta, main="Auto-Corrélation du PIB
      trimestriel stationnarisé")
  pacf(PIBSta, main="Auto-Corrélation partielle du PIB
      trimestriel stationnarisé")
  par(mfrow=c(1,1))
```

```{r, echo=F}
  kpss.test(PIBSta)
  adf.test(PIBSta)
```


Nous nous intéressons aux ACF, PACF, test de KPSS et test de Dickey Fuller augmenté (Figure \ref{fig12}) afin de vérifier si les résidus obtenus à l'aide de la fonction *decompose* sont stationnaires. Au regard de ces différentes informations, nous pouvons conclure à la stationnarité des résidus.

### SMIC

```{r, fig.width=3.5, fig.height=3, fig.cap="\\label{fig15} SMIC trimestriel stationnarisé", echo=F}
  SMICSta <- na.omit(decompose(SMICTrain)$random)
  plot(SMICSta, main="SMIC trimestriel stationnarisé", xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
```

Comme pour la masse salariale, les ACF et PACF de la Figure \ref{fig16} semblent montrer que la série résiduelle pourrait ne pas être stationnaire. Cependant le test de KPSS ainsi que le test de Dickey Fuller augmenté nous permettent de conclure à la stationnarité des résidus.

```{r, fig.height=2.5, fig.cap="\\label{fig16} Fonctions d'autocorrélation du SMIC stationnarisé", echo=F}
  par(mfrow=c(1,2))
  acf(SMICSta, main="Auto-Corrélation du SMIC
      trimestriel stationnarisé")
  pacf(SMICSta, main="Auto-Corrélation partielle du SMIC
      trimestriel stationnarisé")
  par(mfrow=c(1,1))
```

```{r, echo=F}
  kpss.test(SMICSta)
  adf.test(SMICSta)
```
\newpage
### Taux de chômage des femmes

```{r, fig.width=5, fig.height=4, fig.cap="\\label{fig17} Taux de chômage trimestriel stationnarisé", echo=F}
  TCHOFSta <- na.omit(decompose(TCHOFTrain)$random)
  plot(TCHOFSta, main="Taux de chômage des femmes 
       trimestriel stationnarisé", xaxt="n", cex.main=0.8)
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
```

En ce qui concerne le taux de chômage des femmes, en regardant l'ACF, PACF, le test de KPSS et le test de Dickey Fuller augmenté présents en Figure \ref{fig18}, on peut conclure que la série résiduelle est stationnaire.

```{r, fig.height=2.5, fig.cap="\\label{fig18} Fonctions d'autocorrélation du taux de chômage stationnarisé", echo=F}
  par(mfrow=c(1,2), cex.main=0.8)
  acf(TCHOFSta, main="Auto-Corrélation du Taux de
      chômage des femmes
      trimestriel stationnarisé")
  pacf(TCHOFSta, main="Auto-Corrélation partielle
      du Taux de chômage des femmes
      trimestriel stationnarisé")
  par(mfrow=c(1,1))
  kpss.test(TCHOFSta)
  adf.test(TCHOFSta)
```
\newpage
##Corrélation entre les variables stationnarisées

```{r, fig.width=4, fig.height=3, fig.cap="\\label{fig19} Corrélations entre les variables trimestrielles stationnarisées", echo=F}
corrplot(cor(cbind(MSESta, PIBSta, SMICSta, TCHOFSta)), method = "number", type="lower",
         p.mat=cor.mtest(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), 0.95)[[1]], insig="n",
         col=colorRampPalette(c("blue", "light blue", "red"))(50), title = "
         Corrélations entre les variables trimestrielles")
```

```{r, echo=F}
corr <- cor.mtest(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), 0.95)[[1]]
rownames(corr) <- c("MSE","PIB","SMIC","TCHOF")
colnames(corr) <- c("MSE","PIB","SMIC","TCHOF")
corr
```

On s'aperçoit que la transformation de nos séries a permis de supprimer les corrélations entre elles. En effet, la matrice des corrélations présentes en Figure \ref{fig19} nous montre que la corrélation la plus élevée vaut 0.14 ce qui reste très faible. De plus, en regardant le tableau des p-values, l'hypothèse nulle de non significativité du coefficient de corrélation n'est rejetée pour aucun couple de variables (avec un seuil de 5%).

Maintenant que toutes les séries ont été stationnarisées, elles peuvent être utilisées pour construire un modèle VAR.

## Mise en place de modèles VAR avec le package vars

Le package vars a été construit par Bernhard Pfaff. Il permet de construire des modèles vectoriels (VAR), et dispose également des différentes fonctions de diagnostics permettant de vérifier que les hypothèses du modèle sont bien remplies. La version utilisée est la version 1.5-3 et date du 6 Août 2018.

### Calcul de l'ordre p

Afin de mettre en place une modélisation VAR, nous devons dans un premier temps nous intéresser à l'ordre p du modèle VAR. L'ordre p correspond à l'ordre de l'opérateur de retard, c'est-à-dire le nombre de valeurs du passé qui ont un impact sur la valeur à un instant t. Dans le package **vars**, la fonction *VARselect* permet de déterminer l'ordre des modèles VAR à selectionner en fonction de 4 critères (AIC, HQ, SC et FPE).

Pour les critères suivants, p correspond à l'ordre du modèle VAR, T le nombre d'observations utilisées pour la phase d'apprentissage, K le nombre de variables et $\tilde{\Sigma}_u (p) = \frac{1}{T} \Sigma_{t=1}^T \hat{u}_t \hat{u}_t'$ (la matrice de covariance des résidus du modèle).

Dans cette partie, nous développerons le fonctionement de la méthodologie en l'appliquant uniquement au modèle complet, soit celui prenant en compte les variables PIB, SMIC et taux de chômage des femmes.

```{r, fig.height=3.75, fig.cap="\\label{fig20} Critères associés au modèle complet", echo=F}
selec <- VARselect(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), lag.max=10)
par(mfrow=c(2,2))
plot(seq(1:10),t(selec$criteria[1,]), type="l", main="Evolution de l'AIC en
     fonction de l'ordre",
     xlab="Ordre", ylab="AIC")
abline(v=which.min(selec$criteria[1,]), col="blue")
plot(seq(1:10),t(selec$criteria[2,]), type="l", main="Evolution du critère HQ
     en fonction de l'ordre",
     xlab="Ordre", ylab="HQ")
abline(v=which.min(selec$criteria[2,]), col="blue")
plot(seq(1:10),t(selec$criteria[3,]), type="l", main="Evolution du SC en
     fonction de l'ordre",
     xlab="Ordre", ylab="SC")
abline(v=which.min(selec$criteria[3,]), col="blue")
```

Le critère AIC (Aikaike information criterion) se calcule, dans ce package, de la manière suivante : $AIC(p) = \ln \det(\tilde{\Sigma}_u(p)) + \frac{2}{T}p K^2 \quad$. L'objectif est de minimiser ce critère. Cela suppose donc que le déterminant de la matrice $\tilde{\Sigma}_u(p)$ soit strictement positif. Ce critère est asymptotiquement effiace : si le nombre d'observations tend vers l'infini, sa variance est aussi faible que possible.

Le critère HQ (Hannan-Quinn criterion) se calcule, dans ce package, de la manière suivante : $HQ(p) = \ln \det(\tilde{\Sigma}_u(p)) + \frac{2 \ln(\ln(T))}{T}p K^2 \quad$. L'objectif est de minimiser ce critère. Encore une fois, cela suppose que le déterminant de la matrice $\tilde{\Sigma}_u(p)$ soit strictement positif.

Le critère SC (Schwarz criterion) se calcule dans ce package de la manière suivante : $SC(p) = \ln \det(\tilde{\Sigma}_u(p)) + \frac{\ln(T)}{T}p K^2 \quad$. L'objectif est de minimiser ce critère. Ce critère est un autre nom du BIC.

On s'aperçoit que les différents critères à notre disposition, visibles sur les graphiques de la Figure \ref{fig20}, nous donnent des ordres à choisir différents. Ainsi, le meilleur AIC correspond à un modèle d'ordre 10, le meilleur HQ à un modèle d'ordre 3 et le meilleur SC à un modèle d'ordre 2. L'ordre de l'AIC étant trop grand (car trop de coefficients à estimer par rapport au nombre d'observations à notre disposition), nous ne souhaitons pas conserver cet ordre. De plus, on se rend compte que l'AIC du modèle avec un ordre 10 est similaire à celle d'un modèle avec un ordre 4. Les modèles HQ et SC sont meilleurs avec respectivement un ordre 3 et 2. Nous allons donc, dans la suite de l'analyse, essayer les trois modèles définit par les différents critères : ici, nous allons donc nous intéresser aux modèles d'ordre 2, 3 et 4.

### Estimation du modèle VAR(p)

Dans la partie précédente, nous avons sélectionné le meilleur ordre pour notre  modèle VAR. Il s'agit maintenant d'estimer différents modèles afin de pouvoir prédire la MSE. L'exemple que nous avons pris est pour le modèle complet, avec les trois ordres déterminés précédemment (2, 3 et 4). 

Un modèle VAR s'écrit sous la forme suivante : 

  $y_t = \sum_{i = 1}^{p} {A_iy_{t-i}} + u_t$

$A_i$ représentent les matrices de coefficients du modèle pour un ordre i, t le décalage de la série et $u_t$ une matrice K-dimensionnelle composée des résidus du modèle (indépendants et identiquement distribués).

####Ordre 4

Dans le package **vars**, la fonction utilisée pour construire des modèles VAR est VAR, qui prend en entrée la série temporelle multivariée, l'ordre du processus et le type de régresseurs à inclure. Dans notre cas, *type* vaut *const* car la série est stationnarisée et donc centrée en une constante $\mu$. Ci-dessous, le modèle d'ordre 4.

```{r, echo=F}
modele<-VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=4, type="const")
```

Les coefficients du modèle sont les suivants. Les erreurs standards associées sont présentes en \ref{Annexe3}.

```{r, echo=F}
A1<-rbind(modele$varresult$MSESta$coefficients[1:4], 
          modele$varresult$PIBSta$coefficients[1:4], 
          modele$varresult$SMICSta$coefficients[1:4],
          modele$varresult$TCHOFSta$coefficients[1:4])
colnames(A1)<-rownames(A1)<-c("MSE", "PIB", "SMIC", "TCHOF")
A1
A2<-rbind(modele$varresult$MSESta$coefficients[5:8], 
          modele$varresult$PIBSta$coefficients[5:8], 
          modele$varresult$SMICSta$coefficients[5:8],
          modele$varresult$TCHOFSta$coefficients[5:8])
colnames(A2)<-rownames(A2)<-c("MSE", "PIB", "SMIC", "TCHOF")
A2
A3<-rbind(modele$varresult$MSESta$coefficients[9:12], 
          modele$varresult$PIBSta$coefficients[9:12], 
          modele$varresult$SMICSta$coefficients[9:12],
          modele$varresult$TCHOFSta$coefficients[9:12])
colnames(A3)<-rownames(A3)<-c("MSE", "PIB", "SMIC", "TCHOF")
A3
A4<-rbind(modele$varresult$MSESta$coefficients[13:16],
          modele$varresult$PIBSta$coefficients[13:16],
          modele$varresult$SMICSta$coefficients[13:16],
          modele$varresult$TCHOFSta$coefficients[13:16])
colnames(A4)<-rownames(A4)<-c("MSE", "PIB", "SMIC", "TCHOF")
A4

A0<-c(modele$varresult$MSESta$coefficients[17], 
          modele$varresult$PIBSta$coefficients[17], 
          modele$varresult$SMICSta$coefficients[17],
          modele$varresult$TCHOFSta$coefficients[17])
names(A0)<-c("MSE", "PIB", "SMIC", "TCHOF")
```

Les indicateurs de qualité du modèle sont présents ci-dessous.

```{r, echo=F}
selection<-VARselect(cbind(MSESta, PIBSta, SMICSta, TCHOFSta))
selection$criteria[,4]
```

Enfin, l'erreur quadratique moyenne de ce modèle pour les données prédites est la suivante :

```{r, fig.width=4, fig.height=3.5, fig.cap="\\label{fig21} Comparaison entre les vraies valeur et le modèle complet pour un ordre 4", echo=F}
ordre4 <- forecast(VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=4, type="const"))
EQM(MSETest, ordre4$forecast$MSESta$mean*MSETrendTest*MSESeasonalTest)
plot(MSETest, xaxt="n")
axis(side=1, at=seq(2016,2017.25,0.25), labels=c("2016Q1", "2016Q2", "2016Q3", "2016Q4", "2017Q1", "2017Q2"))
lines(ordre4$forecast$MSESta$mean*MSETrendTest*MSESeasonalTest, col="red")
```

La représentation graphique de la Figure \ref{fig21} nous montre des prédictions proches des vraies valeurs, à part pour le premier trimestre 2017.

####Ordre 3

On s'intéresse ensuite au modèle d'ordre 3, soit celui avec le meilleur critère HQ.

```{r, include=F}
modele2<-VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=3, type="const")
```

Les coefficients du modèle associés à un retard de 1 sont les suivants :

```{r, echo=F}
A1modele2<-rbind(modele2$varresult$MSESta$coefficients[1:4], 
          modele2$varresult$PIBSta$coefficients[1:4], 
          modele2$varresult$SMICSta$coefficients[1:4],
          modele2$varresult$TCHOFSta$coefficients[1:4])
colnames(A1modele2)<-rownames(A1modele2)<-c("MSE", "PIB", "SMIC", "TCHOF")
A1modele2
```

Les indicateurs de qualité du modèle sont présents ci-dessous.

```{r, echo=F}
selection<-VARselect(cbind(MSESta, PIBSta, SMICSta, TCHOFSta))
selection$criteria[,3]
```

L'erreur quadratique moyenne de ce modèle pour les données prédites est la suivante :

```{r, fig.width=4.5, fig.height=3.75, fig.cap="\\label{fig22} Comparaison entre les vraies valeur et le modèle complet pour un ordre 3", echo=F}
ordre3 <- forecast(modele2, h=8)
EQM(MSETest, ordre3$forecast$MSESta$mean*MSETrendTest*MSESeasonalTest)
plot(MSETest, xaxt='n')
axis(side=1, at=seq(2016,2017.25,0.25), labels=c("2016Q1", "2016Q2", "2016Q3", "2016Q4", "2017Q1", "2017Q2"))
lines(window(ordre3$forecast$MSESta$mean*MSETrendTest*MSESeasonalTest, start=2016, end=c(2017,2)), col="red")
```

Graphiquement, la Figure \ref{fig22} associée à l'ordre 3 nous donne des prédictions semblant inférieures à celles du modèle d'ordre 4. L'apport d'un lag supplémentaire dans la construction du retard a donc bien une importance dans la qualité des prédictions.

####Ordre 2

Enfin, nous construisons le modèle avec le meilleur BIC, soit celui d'ordre 2.

```{r, include=F}
modele3<-VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=2, type="const")
```

Les coefficients du modèle associés à un retard de 1 sont les suivants :

```{r, echo=F}
A1modele3<-rbind(modele3$varresult$MSESta$coefficients[1:4], 
          modele3$varresult$PIBSta$coefficients[1:4], 
          modele3$varresult$SMICSta$coefficients[1:4],
          modele3$varresult$TCHOFSta$coefficients[1:4])
colnames(A1modele3)<-rownames(A1modele3)<-c("MSE", "PIB", "SMIC", "TCHOF")
A1modele3
```

Les indicateurs de qualité du modèle sont présents ci-dessous.

```{r, echo=F}
selection<-VARselect(cbind(MSESta, PIBSta, SMICSta, TCHOFSta))
selection$criteria[,2]
```

L'erreur quadratique moyenne de ce modèle pour les données prédites est la suivante :

```{r, fig.width=5, fig.height=4, fig.cap="\\label{fig23} Comparaison entre les vraies valeur et le modèle complet pour un ordre 2", echo=F}
ordre2 <- forecast(modele3, h=8)
EQM(MSETest, ordre2$forecast$MSESta$mean*MSETrendTest*MSESeasonalTest)
plot(MSETest, xaxt="n")
axis(side=1, at=seq(2016,2017.25,0.25), labels=c("2016Q1", "2016Q2", "2016Q3", "2016Q4", "2017Q1", "2017Q2"))
lines(window(ordre2$forecast$MSESta$mean*MSETrendTest*MSESeasonalTest, start=2016, end=c(2017,2)), col="red")
```

Lorsque l'on compare les erreurs quadratiques moyennes, nous remarquons que la plus faible est celle associée à un modèle d'ordre 4, soit celui avec le meilleur AIC. Il nous faut maintenant vérifier que les hypothèses associées à ce modèle soient bien vérifiées.

### Verification de la stabilité

Pour vérifier si le processus VAR est stable, c'est-à-dire qu'il génère des séries stationnaires, nous devons calculer les valeurs propres de la matrice des coefficients : 

$$A = \begin{bmatrix}
A_1 & A_2 & \cdots & A_{p-1} & A_p \\
I & 0 & \cdots & 0 & 0 \\
0 & I & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & I & 0 
\end{bmatrix}$$

Si les modules des valeurs propres de A sont inférieures à 1, alors le processus VAR est stable. Nous allons donc vérifier que le processus VAR(4) créé précédemment avec toutes les variables à notre disposition est bien stable.

```{r, echo=F}
#Construction de la matrice A
A <- matrix(0,nrow=16, ncol=16)
A[1:4,1:4] = A1
A[1:4,5:8] = A2
A[1:4,9:12] = A3
A[1:4,13:16] = A4
A[5:8,1:4] = diag(1,4,4)
A[9:12,5:8] = diag(1,4,4)
A[13:16,9:12] = diag(1,4,4)
#Calcul des valeurs propres
vp <- eigen(A)
Mod(vp$values)
plot(seq(1,16), Mod(vp$values), xlab="",
     ylab="Modules des valeurs propres", ylim=c(0,1.5))
abline(h=1, col="red")
```

On s'aperçoit que tous les modules sont inférieurs à 1, le processus VAR(4) est donc stable.

### Test ARCH (homoscédasticité des résidus)

Le test multivarié de ARCH-LM permet de tester l'homoscédasticité des résidus. La statistique de test est la suivante : $VARCH_{LM}(q) = \frac{1}{2}TK(K+1)R_m^2$, où $R_m^2 = 1 - \frac{2}{K(K+1)}tr(\hat{\Omega}\hat{\Omega}_O^{-1})$, et $\hat{\Omega}$ est la matrice de covariance de la régression suivante : $vech(\hat{u_t}\hat{u_t}^T) = \beta_0 + B_1 vech(\hat{u}_{t-1}\hat{u}_{t-1}^T) + ... + B_q vech(\hat{u}_{t-q}\hat{u}_{t-q}^T) + v_t$. La dimension de $\beta_O$ est $\frac{1}{2}K(K+1)$ et celle des matrices des coefficients $B_i$ est $\frac{1}{2}K(K+1) × \frac{1}{2}K(K+1)$. La statistique de test suit une loi de $\chi^2(qK^2(K+1)^2/4)$, donc dans notre cas $\chi^2(16q*25/4)$. L'hypothèse nulle de ce test est $H_0 : B_0 = B_1 = ... = B_q = 0$ (homoscédasticité).

```{r, fig.width=5, fig.height=4, fig.cap="\\label{fig24} Evolution de la p-value en fonction du lag", echo=F}
a1 <- c()
for(i in 1:18){
a1[i] <- arch.test(modele, lags.multi = i)$arch.mul$p.value
}
plot(a1, type="l", xlab="lag", ylab="p-value")
abline(h=0.05, col="red")
```

On s'aperçoit au regard de la figure \ref{fig24}, avec un seuil de confiance de 5% (ligne rouge), qu'on rejette l'hypothèse nulle d'homoscédasticité pour un retard faible (inférieur à 7). Cependant, en augmentant le nombre de valeurs prises en compte pour calculer la nouvelle, on se rend compte qu'on conserve l'hypothèse d'homoscédasticité. On observe également que la valeur de la p-value converge vers 1 au fur et à mesure qu'on augmente le retard. Ainsi, en prenant l'ensemble des résidus, nous conservons l'hypothèse d'homoscédasticité.

### Test normalité (normalité des résidus)

Le test de Jarque-Bera pour séries multivariées permet de tester la normalité des résidus. Il utilise les résidus standardisés à l'aide d'une décomposition de Cholesky de la matrice de variance-covariance des résidus centrés. Il est important noter que l'ordre dans lequel les variables sont stockées dans la matrice a une importance sur les résultats. La statistique de test est la suivante : $JB_{mv} = s_3^2 + s_4^2$, où $s_3^2$ et $s_4^2$ se calculent de la sorte : $s_3^2 = Tb_1^Tb_1/6$ et $s_4^2 = T(b_2 - 3_K)^T(b_2-3_K)/24$, avec $b_1$ et  $b_2$ qui sont respectivement les vecteurs des moments non-centrés d'ordre trois et quatre des résidus standardisés. La statistique de test suit une loi de $\chi^2(2K)$. Ce test compare en fait le coefficient kurtosis K (l'aplatissement de la fonction de densité) et le coefficient skewness S (asymétrie de la fonction de densité) d'une loi normale à ceux des résidus testés. L'hypothèse nulle est donc $H0 : S = 0$ et $K = 3$.

```{r, echo=F}
normality.test(modele)$jb.mul$JB
```

Ici, on rejette l'hypothèse H0, avec un seuil de confiance de 5%. Les résidus obtenus ne suivent pas une loi normale.

### Test Portmanteau (corrélations des résidus)

Le test de Portmanteau multivarié permet de tester l'auto-corrélation (au sein d'une même série) et la corrélation croisée (entre les différentes séries) des résidus.

#### Verification de la Matrice $C_0$

La statistique de Portmanteau est $Q_h = T \sum_{j=1}^h tr(\hat{C}_j^T \hat{C}_0^{-1} \hat{C}_j \hat{C}_0^{-1})$, et elle suit une loi de $\chi^2(K^2h - n^*)$, $n^*$ étant le nombre de coefficients à estimer. Pour qu'elle existe, il faut donc vérifier que la matrice $\hat{C}_0$ est inversible pour que la statistique puisse être définie. Les matrices $\hat{C}_i$ s'écrivent $\hat{C}_i = \frac{1}{T} \sum_{t=i+1}^T \hat{u}_t \hat{u}_{t-i}^T$, donc $\hat{C}_0$ s'écrit $\hat{C}_0 = \frac{1}{T} \sum_{t=1}^T \hat{u}_t \hat{u}_t^T$. Nous allons donc vérifier qu'elle est inversible pour le modèle complet que nous avons mis en place. $tr()$ correspond à la trace de la matrice, soit la somme des éléments diagonaux de la matrice.

```{r, echo=F}
C0 <- matrix(nrow = 4, ncol=4, 0)
for(i in 1:nrow(residuals(modele))){
  C0 <- C0 + residuals(modele)[i,]%*%t(residuals(modele)[i,])
}
C0 <- (1/nrow(residuals(modele))) * C0
C0
d <- det(C0)
names(d) <- "Déterminant de la matrice"
d
```

Le déterminant de la matrice n'étant pas nul, la matrice $\hat{C}_0$ est donc inversible.

#### Application du test

Les 3 premières p-values ne peuvent être calculées à cause de la valeur des degrés de liberté. En effet, comme nous l'avons expliqué plus haut, la statistique de test suit une loi de $\chi^2(K^2h - n^*)$. Or, avec un retard compris entre 1 et 3, les degrés de liberté sont négatifs et il n'est donc pas possible d'appliquer le test. L'hypothèse nulle de ce test est l'absence de corrélations croisées et d'auto-corrélations.

```{r, , fig.width=5, fig.height=4, fig.cap="\\label{fig24} Evolution de la p-value en fonction du lag", echo=F}
a1 <- c()
for(i in 1:3){
  a1[i] <- NA
} 
for(i in 4:50){
  a1[i] <- serial.test(modele, lags.pt=i, type="PT.asymptotic")$serial$p.value
}
plot(a1, type="l", main="Evolution de la p-value en fonction du lag", xlab="lag", ylab="p-value")
abline(h=0.05, col="red")
```

Au regarde la figure \ref{fig21} Comme pour le test ARCH, on rejette l'hypothèse nulle, avec un seuil de confiance à 5% (ligne rouge) pour un retard faible (5 ou moins). Cependant, pour un retard grand (supérieur à 5), on conserve l'hypothèse nulle d'absence d'auto-corrélations et de corrélations croisées. On observe également que la p-value converge vers 1 à mesure qu'on augmente le retard. Ainsi, en prenant en compte l'ensemble des résidus, on conserve l'hypothèse d'absence d'auto-corrélations et de corrélations croisées.

### Prévisions

Maintenant que nous avons estimé l'ordre des différents modèle VAR, et que nous avons explicité l'estimation des modèles, nous cherchons désormais à trouver celui dont les prédictions sont les plus proches de la réalité.

Après avoir comparé tous les modèles possibles (7 : 3 modèles avec deux variables, 3 modèles avec trois variables et un modèle avec les quatre variables), nous nous apercevons que le meilleur en terme de prédictions est le modèle prenant en compte le SMIC (en plus de la masse salariale).

```{r}
#SMIC
VARselect(cbind(MSESta, PIBSta), lag.max=10)
VAR(cbind(MSESta, PIBSta), p=4, type="const")
VARSMICSta <- forecast(VAR(cbind(MSESta, PIBSta), p=4, type="const"))
plot(MSETest, xlim=c(2016,2017.25), main="Différences entre les véritables
     valeurs de 2016 et les prédictions du modèle pour la masse salariale", xaxt="n")
axis(side=1, at=seq(2016,2017.25,0.25), labels=c("2016Q1", "2016Q2", "2016Q3", "2016Q4", "2017Q1", "2017Q2"))
lines(window(VARSMICSta$forecast$MSESta$mean*MSETrendTest*MSESeasonalTest, start=2016, end=c(2017,2)), col = "red")
legend('bottomleft', legend = c('Vraies valeurs', 'Prévisions du modèle'),
       col=c('black', 'red'), lty=1, cex=0.8)
```

Nous nous intéressons donc à l'erreur quadratique moyenne de cette prévision.

```{r}
EQM(MSETest, window(VARSMICSta$forecast$MSESta$mean*MSETrendTest*MSESeasonalTest, start=2016, end=c(2017,2)))
```

\appendix

#Annexe 1 : Modélisation univariée des séries \label{Annexe1}

##Modélisation individuelle \label{MI}

Une fois que nous avons analysé le comportement des différentes séries temporelles à notre disposition, nous souhaitons les modéliser afin de prédire les valeurs futures de ces différentes séries. En effet, si nous voulons prédire la MSE pour des valeurs futures, nous aurons également besoin des valeurs associées pour les variables explicatives, qui ne seront peut-être pas à notre disposition. Nous avons utilisé à la fois des modèles basés sur un lissage exponentiel et des processus ARMA.

###Comparaison des différents modèles
Afin de comparer les modèles construits pour chaque série avec les différentes méthodes, nous calculons l'Erreur Quadratique Moyenne (EQM), soit les moyennes des différences au carré entre les valeurs de test et les valeurs prédites par le modèle.

###Lissage exponentiel

####Définition

Le lissage exponentiel permet de prédire les valeurs d’une série temporelle en lissant successivement les données à partir d’une valeur initiale. Plus les observations sont éloignées dans le passé, moins leur poids est important lors du calcul. Pour une série stationnaire, la formule de calcul d’une valeur est la suivante :  $s_t =  \alpha y_t + (1-\alpha) s_{t-1}$, le paramètre $\alpha$ étant le facteur de lissage. Le nom de cette méthode est un lissage exponentiel **simple**. Afin de modéliser les séries possédant une tendance, nous introduisons un paramètre $\beta$ permettant de la prendre en compte, la méthode étant appelée lissage exponentiel **double**. Enfin, Holt et Winters ont également modifié la méthode pour qu’elle puisse modéliser les séries comportant une saisonnalité en introduisant un paramètre $\gamma$. Ils ont donné leur nom à cette méthode, qui est donc un lissage exponentiel de **Holt-Winters**.

Dans notre cas, nous ne calculons pas nous-mêmes $\alpha$, $\beta$ et $\gamma$ Ces paramètres sont déterminés automatiquement par la fonction *ets* du package **forecast** de façon à optimiser la qualité de la prédiction. Cette fonction permet également de choisir la méthode à utiliser, grâce à l'argument model. Afin de mesurer la qualité de notre modèle, nous avons choisi d'utiliser l’**AICc** (Akaike Information Criterion with correction). Le choix de l’AICc par rapport à l’AIC s’explique par le faible nombre de données que nous possédons par rapport au nombre de paramètres à estimer. C'est ce critère qui nous servira par la suite afin de comparer nos différents modèles. \label{AICc}

Prenons l'exemple de la MSE. Nous avons vu dans la partie \ref{MSE} que la série possédait une tendance linéaire ainsi qu'une saisonnalité multiplicative. L'argument model de la fonction *ets** prendra donc la valeur "ZAM", (erreur sélectionnée automatiquement, tendance linéaire, saisonnalité multiplicative). On peut également remarquer que lorsque tous les paramètres sont automatiquement sélectionnés (valeur "ZZZ"), les paramètres retenus sont les mêmes que ceux que nous avions rentré.

```{r, fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig10} Comparaison entre la prédiction du lissage exponentiel et les valeurs réelles pour la masse salariale trimestrielle", echo=F}
LEMSE<-ets(MSETrain, "ZAM")
print(LEMSE)
PredLEMSE <- forecast(LEMSE, h = 6)
plot(MSETest, main="")
lines(PredLEMSE$mean, col="red")
EQM(MSETest, PredLEMSE$mean)
```

On obtient donc un AICc de 4033.451 pour le modèle ainsi qu'une erreur quadratique moyenne de $2.6*10^{14}$. Le graphique obtenu en figure \ref{fig10} nous montrent que le modèle obtenu nous donne des prédictions très proches de la réalité.

####Résultats obtenus

```{r, include=F}
LEPIB<-ets(PIBTrain, "ZAN")
print(LEPIB)
PredLEPIB <- forecast(LEPIB, h = 5)
EQM(PIBTest, PredLEPIB$mean)

LESMIC<-ets(SMICTrain, "ZAA")
print(LESMIC)
PredLESMIC <- forecast(LESMIC, h = 6)
EQM(SMICTest, PredLESMIC$mean)

LETCHOF<-ets(TCHOFTrain, "ZNN")
print(LETCHOF)
PredLETCHOF <- forecast(LETCHOF, h = 6)
EQM(TCHOFTest, PredLETCHOF$mean)
```

```{r, fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig11} Résultats obtenus pour le lissage exponentiel", echo=F}
par(mfrow=c(2,2))
plot(PIBTest, ylim=c(min(PIBTest,PredLEPIB$mean),max(PIBTest,PredLEPIB$mean)), main=
"Comparaison entre la prédiction du lissage 
exponentiel et les valeurs réelles pour le PIB 
trimestriel", cex.main=0.8)
lines(PredLEPIB$mean, col="red")

plot(SMICTest, ylim=c(min(SMICTest,PredLESMIC$mean),max(SMICTest,PredLESMIC$mean)), 
main="Comparaison entre la prédiction du lissage 
exponentiel et les valeurs réelles pour le SMIC 
trimestriel", cex.main=0.8)
lines(PredLESMIC$mean, col="red")

plot(TCHOFTest, ylim=c(min(TCHOFTest,PredLETCHOF$mean),max(TCHOFTest,PredLETCHOF$mean)), 
main="Comparaison entre la prédiction du lissage 
exponentiel et les valeurs réelles pour le taux 
de chômage trimestriel", cex.main=0.8)
lines(PredLETCHOF$mean, col="red")
```
Les graphiques de la figure \ref{fig11} nous montrent des résultats mitigés. Pour le PIB et le SMIC, les prédictions suivent la forme de la série mais en sont éloignées. Pour le taux de chômage des femmes, la méthode de lissage utilisée est un lissage exponentiel simple, ce qui nous donne donc des prédictions constantes soit de mauvaise qualité. 

Nous résumons dans le tableau suivant les résultats obtenus pour chaque série estimée par un lissage exponentiel.

| Variable | Tendance |  Saisonnalité  | Argument model |   AIC   |
| -------- | -------- | -------------- | -------------- | ------- |
| MSE      | linéaire | multiplicative |      ZAM       | 4033.45 |
| PIB      | linéaire | absente        |      ZAN       | 2053.15 |
| SMIC     | linéaire | additive       |      ZAA       | -84.96  |
| TCHOF    | absente  | absente        |      ZNN       | 204.37  |
 
###Modèles ARMA

####Définition

Les modèles **ARMA(p,q)** sont une autre famille de modèles permettant d’estimer une série temporelle. Il est divisé en deux parties : une partie  autorégressive **AR** auquel est associé un ordre *p* qui donne le nombre de valeurs passées qui vont être utiles dans la prédiction, et une partie moyennes mobiles **MA** qui permet de de prendre en compte les *q* innovations de la série dans le futur.

L’une des propriétés des processus ARMA est qu’ils sont utilisés pour modéliser des séries stationnaires, donc par extension des séries qui ne possèdent ni tendance ni saisonnalité. Afin de modéliser des séries non stationnaires, on généralise les processus ARMA en processus **ARIMA(p,d,q)**, *d* représentant l’ordre de différenciation de la série. Les séries saisonnières sont elles modélisées par des processus **$SARIMA(p ,d, q)(P, D, Q)_s$** qui modélisent des séries avec une saisonnalité de période *s*.

Comme pour le lissage exponentiel, nous ne calculons pas nous-mêmes les ordres des processus. Pour cela, la fonction *auto.arima* du package **forecast** nous a été très utile. Elle permet en effet de trouver les ordres du processus qui optimisent un critère défini à l’avance et de calculer un modèle avec ces coefficients. Nous avons choisi d’optimiser l’**AICc**(Akaike Information Criterion with correction), pour les raisons évoquées dans la partie \ref{AICc}

```{r, fig.width=4, fig.height=3.5, fig.align='center', fig.cap="\\label{fig12} Comparaison entre le modèle SARIMA et les données de validation pour la masse salariale trimestrielle", echo=F}
ARIMAMSE<-auto.arima(MSETrain, ic="aicc")
print(ARIMAMSE)
PredARIMAMSE<- forecast(ARIMAMSE, h=6)
plot(MSETest, main="")
lines(PredARIMAMSE$mean, col="red")
```

Pour la MSE, on obtient par exemple un modèle $SARIMA(0,1,1)(0,1,1)_4$ ainsi qu'un AICc de 3896.01. La  figure \ref{fig12} nous donne des prédictions d'assez bonne qualité mais qui semblent moins bonnes que celles obtenues par lissage exponentiel.

####Résultats obtenus
```{r, fig.cap="\\label{fig13} Résultats obtenus avec des modèles ARMA", echo=F}
par(mfrow=c(2,2))
ARIMAPIB<-auto.arima(PIBTrain, ic="aicc", seasonal=F)
print(ARIMAPIB)
PredARIMAPIB<- forecast(ARIMAPIB, h=5)
plot(PIBTest, ylim=c(min(PIBTest,PredARIMAPIB$mean),max(PIBTest,PredARIMAPIB$mean)), 
main="Comparaison entre le modèle SARIMA et les données de
    validation pour le PIB trimestriel", cex.main=0.8)
lines(PredARIMAPIB$mean, col="red")

ARIMASMIC<-auto.arima(SMICTrain, ic="aicc")
print(ARIMASMIC)
PredARIMASMIC<- forecast(ARIMASMIC, h=6)
plot(SMICTest, ylim=c(min(SMICTest,PredARIMASMIC$mean),max(SMICTest,PredARIMASMIC$mean)), 
main="Comparaison entre le modèle SARIMA et les données de
    validation pour le SMIC trimestriel", cex.main=0.8)
lines(PredARIMASMIC$mean, col="red")

ARIMATCHOF<-auto.arima(TCHOFTrain, ic="aicc", seasonal=F)
print(ARIMATCHOF)
PredARIMATCHOF<- forecast(ARIMATCHOF, h=6)
plot(TCHOFTest, main="Comparaison entre le modèle SARIMA et les données de
    validation pour le taux de chômage des femmes trimestriel", cex.main=0.8)
lines(PredARIMATCHOF$mean, col="red")
```

Comme pour le lissage exponentiel, nous résumons les résultats obtenus dans un tableau pour plus de lisibilité. Le PIB et le taux de chômage des femmes ne comportent pas de partie saisonnière car comme vu dans les parties \ref{PIB} et \ref{TCHOF} on ne constate pas de saisonnalité dans l'analyse descriptive de la série. On peut également voir sur la figure \ref{fig13} que les prédictions du PIB semblent de bien meilleure qualité

| Variable | Ordre du processus |  AICc   |
| -------- | -------------------| ------- |
| MSE      |   (0,1,1)(0,1,1)   | 3675.34 |
| PIB      |       (2,1,0)      | 1839.36 |
| SMIC     |   (1,0,0)(1,1,0)   | -261.47 |
| TCHOF    |       (0,1,1)      |  9.76   |

###Comparaison des différents modèles

Une fois que nous avons construit les deux types de modèles pour chacune des variables, nous souhaitons les comparer pour savoir quel modèle est le plus efficace pour prédire chacune des variables. Pour cela, les EQM, calculant l'erreur de prédiction, de chacun des modèles sont synthétisées dans le tableau suivant. L'AICc ne peut pas être utilsié ici car les méthodes à comparer sont différentes. Il n'est donc pas sûr que la méthode utilisée pour calculer la vraisemblance soit la même.

```{r, echo=F}
resultats<-matrix(nrow=4, ncol=2, dimnames = list(c("MSE", "PIB", "SMIC", "TCHOF"), 
                                                  c("lissage", "ARMA")))
resultats[1,1] = EQM(MSETest, PredLEMSE$mean)
resultats[2,1] = EQM(PIBTest, PredLEPIB$mean)
resultats[3,1] = EQM(SMICTest, PredLESMIC$mean)
resultats[4,1] = EQM(TCHOFTest, PredLETCHOF$mean)
resultats[1,2] = EQM(MSETest, PredARIMAMSE$mean)
resultats[2,2] = EQM(PIBTest, PredARIMAPIB$mean)
resultats[3,2] = EQM(SMICTest, PredARIMASMIC$mean)
resultats[4,2] = EQM(TCHOFTest, PredARIMATCHOF$mean)
resultats
```

Nous nous rendons compte que le lissage a une EQM plus faible pour la masse salariale (notre variable d’intérêt), ainsi que pour le SMIC et le taux de chômage des femmes. En ce qui concerne le PIB, le modèle ARIMA est plus performant. Cette analyse va nous servir par la suite, comme expliqué au début de la partie \ref{MI}

## Modélisation ARMA avec variables exogènes \label{Annexe2}

### Définition

Maintenant que nous avons modélisé chaque série individuellement, nous souhaitons savoir s’il est possible d’améliorer la qualité de prédiction de la série MSE trimestrielle à l’aide des autres variables à notre disposition. Pour ce faire, nous allons construire des modèles SARIMA prenant en compte des variables exogènes. 

$Y_{t} = \beta_{0} +\beta_{1}X_{1t} + ... + \beta_{k}X_{kt} + \epsilon_{t}$

$Y_{t}$ est la variable à modéliser. $X_{i}$ pour $i = 1, ..., k$ correspond à la $i$ème variable exogène. $\beta_{i}$ pour i allant de $i = 0, ..., k$ correspond aux coefficients d'une régression linéaire. Enfin, le résidu $\epsilon_{t}$ suit un processus de type ARMA.

```{r, fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig14} Masse salariale expliquée par le PIB, le SMIC et le taux de chômage vs Vraies valeurs", echo=F}
#PIB & SMIC & TCHO
SARIMACOMPLET <- auto.arima(MSETrain, xreg = cbind(PIBTrain, SMICTrain, TCHOFTrain))
PredCOMPLET <- forecast(SARIMACOMPLET, xreg = cbind(PIBTest, SMICTest, TCHOFTest))
plot(PredCOMPLET$mean, col="red",
     ylim=c(min(MSETest,PredCOMPLET$mean), max(MSETest,PredCOMPLET$mean)),
     main = "")
lines(MSETest)
EQM(PredCOMPLET$mean, MSETest)
```

Ici, les résidus suivent un SARIMA(0,0,0)(0,1,0)[4], et le modèle possède 3 variables exogènes : le coefficient correspondant à la variable PIB est $\beta_1 = 706.0045$, celui correspondant à la variable SMIC est $\beta_2 = 209266766$ et enfin celui correspondant au taux de chômage est $\beta_3 = -1644706$

### Résultats obtenus

Nous allons désormais nous intéresser à la construction des différents modèles prenant en compte 1 variable exogène (3 modèles), 2 variables exogènes (3 modèles) et 3 variables exogènes (1 modèle). La qualité de ces 7 modèles est representée dans le tableau ci-dessous.

| Variable     | Ordre du processus |  AICc   |
| ------------ | -------------------| ------- |
| Aucune       |   (0,1,1)(0,1,1)   | 3675.34 |
| PIB          |   (1,0,0)(2,1,0)   | 3715.99 |
| SMIC         |   (0,1,0)(0,1,0)   | 3688.92 |
| TCHOF        |   (0,0,0)(1,1,0)   | 3809.82 |
| PIB & SMIC   |   (0,0,0)(0,1,0)   | 3807.37 |
| PIB & TCHOF  |   (1,0,0)(0,1,0)   | 3721.36 |
| SMIC & TCHOF |   (0,1,0)(0,1,0)   | 3690.57 |
| COMPLET      |   (0,0,0)(0,1,0)   | 3809.5  |

On se rend compte qu'aucun modèle ARIMA ne prenant en compte des variables exogènes n'a une qualité meilleure que celui ne prenant en compte aucune variable exogène (en comparant les AIC corrigés). Si on omet le modèle sans variable exogène, le meilleur modèle prenant en compte au moins une variable exogène est celui prenant en compte le SMIC (dans la figure \ref{fig15}).

```{r, fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig15} SARIMA expliqué par le SMIC vs Vraies valeurs", echo=F}
#SMIC
SARIMASMIC <- auto.arima(MSETrain, xreg = SMICTrain)
PredSMIC <- forecast(SARIMASMIC, xreg = SMICTest)
plot(PredSMIC$mean, col="red",
     ylim=c(min(MSETest,PredSMIC$mean), max(MSETest,PredSMIC$mean)))
lines(MSETest)
EQM(PredSMIC$mean, MSETest)
```

#Annexe 3 : Erreurs standard associées aux coefficients du modèle VAR d'ordre 4 \label{Annexe3}

```{r, echo=FALSE}
#Calcul des erreurs standards associées aux matrices A1, A2, A3 et A4
A1stand<-rbind(summary(modele$varresult$MSESta)$coefficients[1:4,2], 
          summary(modele$varresult$PIBSta)$coefficients[1:4,2], 
          summary(modele$varresult$SMICSta)$coefficients[1:4,2],
          summary(modele$varresult$TCHOFSta)$coefficients[1:4,2])
colnames(A1stand)<-rownames(A1stand)<-c("MSE", "PIB", "SMIC", "TCHOF")
A2stand<-rbind(summary(modele$varresult$MSESta)$coefficients[5:8,2], 
          summary(modele$varresult$PIBSta)$coefficients[5:8,2], 
          summary(modele$varresult$SMICSta)$coefficients[5:8,2],
          summary(modele$varresult$TCHOFSta)$coefficients[5:8,2])
colnames(A2stand)<-rownames(A2stand)<-c("MSE", "PIB", "SMIC", "TCHOF")
A2
A3stand<-rbind(summary(modele$varresult$MSESta)$coefficients[9:12,2],
          summary(modele$varresult$PIBSta)$coefficients[9:12,2],
          summary(modele$varresult$SMICSta)$coefficients[9:12,2],
          summary(modele$varresult$TCHOFSta)$coefficients[9:12,2])
colnames(A3stand)<-rownames(A3stand)<-c("MSE", "PIB", "SMIC", "TCHOF")
A3
A4stand<-rbind(summary(modele$varresult$MSESta)$coefficients[13:16,2],
          summary(modele$varresult$PIBSta)$coefficients[13:16,2],
          summary(modele$varresult$SMICSta)$coefficients[13:16,2],
          summary(modele$varresult$TCHOFSta)$coefficients[13:16,2])
colnames(A4stand)<-rownames(A4stand)<-c("MSE", "PIB", "SMIC", "TCHOF")
A4

A0stand<-c(summary(modele$varresult$MSESta)$coefficients[17,2], 
          summary(modele$varresult$PIBSta)$coefficients[17,2], 
          summary(modele$varresult$SMICSta)$coefficients[17,2],
          summary(modele$varresult$TCHOFSta)$coefficients[17,2])
names(A0stand)<-c("MSE", "PIB", "SMIC", "TCHOF")
```