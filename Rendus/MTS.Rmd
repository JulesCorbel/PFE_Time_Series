---
title: "Package MTS"
author: "Jules CORBEL & Paul GUILLOTTE"
date: "04/02/2019"
output: pdf_document
---

```{r setup, include=FALSE}
require(tseries)
require(forecast)
require(corrplot)
require(fUnitRoots)
require(MTS)
require(portes)

#Code permettant la mise en place de la représentation visuelle de la matrice des corrélations
cor.mtest <- function(mat, conf.level = 0.95){
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
  diag(p.mat) <- 0
  diag(lowCI.mat) <- diag(uppCI.mat) <- 1
  for(i in 1:(n-1)){
    for(j in (i+1):n){
      tmp <- cor.test(mat[,i], mat[,j], conf.level = conf.level)
      p.mat[i,j] <- p.mat[j,i] <- tmp$p.value
      lowCI.mat[i,j] <- lowCI.mat[j,i] <- tmp$conf.int[1]
      uppCI.mat[i,j] <- uppCI.mat[j,i] <- tmp$conf.int[2]
    }
  }
  return(list(p.mat, lowCI.mat, uppCI.mat))
}

EQM<-function(serie, prediction){
  return(sum((serie - prediction)^2)/length(serie))
}
```

Nous nous intéresserons dans ce document à la mise en place de modèles VAR afin de prédire la masse salariale trimestrielle. Un modèle VAR, pour  Vecteur AutoRégressif, a pour objectif de capturer les interdépendances entre les différentes séries temporelles à notre disposition. Ainsi, chaque variable est expliquée par ses propres valeurs passées ainsi que par les valeurs passées des autres variables du modèle.

# Visualation des séries

Nous nous intéressons dans cette partie aux différentes séries trimestrielles à notre disposition. Dans un premier temps, nous nous intéressons aux corrélations entre les variables deux à deux afin de nous faire une première idée du lien qu'il existe entre les variables.

```{r}
trim <- read.csv("~/PFE_Time_Series/Data/Data_Trim.csv", sep=";", dec=",")
corrplot(cor(trim[1:109,-1]), method = "number", type="lower",
         p.mat=cor.mtest(trim[1:109,-1], 0.95)[[1]], insig="pch",
         col=colorRampPalette(c("blue", "light blue", "red"))(50), title = "
         Corrélations entre les variables trimestrielles")
```

On se rend compte que le taux de chômage des femmes est corrélé négativement avec toutes les autres variables. Le trio de variables PIB, masse salariale et SMIC sont extrêmement liées entre elles.

Nous allons maintenant nous attarder sur chaque série individuellement.

##Masse salariale

```{r}
  MSE <- ts(trim$MSE, start = 1990, end = c(2017, 2), frequency=4)
  plot(MSE, main="Evolution de la masse salariale trimestrielle")
  par(mfrow=c(1,2))
  acf(MSE, main="Auto-corrélation de la
      masse salariale trimestrielle", lag.max=20)
  pacf(MSE, main="Autocorrélation partielle
       de la masse trimestrielle", lag.max=20)
  kpss.test(MSE)
```

La masse salariale trimestrielle possède une composante de tendance de 1990 à 2010. La série tend par la suite à stagner. Nous remarquons également une saisonnalité sur cette série, qui est de plus en plus marquée à mesure que le temps passe. 

Comme la série comporte une tendance et une saisonnalité, elle ne correspond pas aux deux premières conditions de la stationnarité du second ordre, soit que la série possède une moyenne et un écart-type constants. Cela est confirmé par la fonction ACF qui décroît régulièrement. Nous effectuons également un test de KPSS servant à vérifier si la série est stationnaire ou non (sous l'hypothèse $H_{0}$ la série est stationnaire, et sous l'hypothèse $H_{1}$ elle ne l'est pas). La p-value est de 0.01 ce qui nous confirme que la série n'est pas stationnaire avec un risque de première espèce de 5%.

##PIB 

```{r}
  PIB <- ts(trim$PIB, start = 1990, end = c(2017, 1), frequency=4)
  plot(PIB, main="Evolution du PIB trimestriel")
  par(mfrow=c(1,2))
  acf(PIB, main="Auto-corrélation 
      du PIB trimestriel", lag.max=20)
  pacf(PIB, main="Autocorrélation partielle
       du PIB trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(PIB)
```

Comme pour la masse salariale, le PIB annuel possède une tendance. Cependant, il ne semble pas posséder de saisonnalité. Cette série n'est donc pas non plus stationnaire. Nous effectuons à nouveau un test de KPSS. La p-value est de 0.01 ce qui nous confirme que la série n'est pas stationnaire avec un risque de première espèce de 5%.

##SMIC
```{r}
  SMIC <- ts(trim$SMIC, start = c(1990,1), end = c(2017, 4), frequency = 4)
  plot(SMIC, main="Evolution du SMIC trimestriel")
  par(mfrow=c(1,2))
  acf(SMIC, main="Auto-corrélation du
      SMIC trimestriel", lag.max=20)
  pacf(SMIC, main="Autocorrélation partielle
       du SMIC trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(SMIC)
```

Au regard de la représentation graphique, on s'aperçoit qu'il y a bien une tendance. Pour la saisonnalité, il est plus difficile de savoir s'il en existe une ou pas, puisque la série semble augmenter seulement à certains temps.

##Taux de chômage des femmes

```{r}
  TCHOF <- ts(trim$TCHOF, start = c(1990,1), end = c(2017, 4), frequency = 4)
  plot(TCHOF, main="Evolution du taux de chômage des femmes trimestriel")
  par(mfrow=c(1,2))
  acf(TCHOF, main="Auto-corrélation du taux de
      chômage des femmes trimestriel", lag.max=20)
  pacf(TCHOF, main="Autocorrélation partielle du
       taux de chômage des femmes trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(TCHOF)
```

Pour cette dernière série qui représente le taux de chômage trimestriel des femmes, il ne semble pas y avoir de saisonnalité. On remarque cependant qu'il y a bien une tendance. Le test KPSS nous confirme que la série n'est pas stationnaire.

# Transformation des séries

Nous allons maintenant transformer les séries pour les rendre stationnaires, afin de pouvoir appliquer les différents modèles ensuite. Afin de stationnariser les séries, nous utiliserons la fonction decompose qui permet de découper la série en trois : la tendance, la saisonnalité et les résidus, afin de pouvoir ensuite travailler avec les résidus.

Pour chacune de ces séries, nous allons créé un échantillon d'apprentissage, qui nous permettra de construire les différents modèles, ainsi qu'un échantillon de test, qui nous permettra de comparer les prédictions des modèles construits avec des vraies valeurs. L'échantillon d'apprentissage sera composé de toutes les valeurs jusqu'au 4e trimestre 2015, et celui de test de toutes les valeurs à partir du 1er trimestre 2016.

## Masse salariale

```{r}
  plot(decompose(MSE, "multiplicative"))
  MSESta <- na.omit(decompose(MSE, "multiplicative")$random)
  MSETrendTest<-window(decompose(MSE, "multiplicative")$trend, start=2016, end=c(2016,4))
  MSESeasonalTest<-window(decompose(MSE, "multiplicative")$seasonal, start=2016, end=c(2016,4))
  par(mfrow=c(1,2))
  acf(MSESta, main="Auto-Corrélation de la Masse
      salariale trimestrielle stationnarisée")
  pacf(MSESta, main="Auto-Corrélation partielle de la Masse
       salariale trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(MSESta)
  plot(MSESta, main="Masse salariale trimestrielle stationnarisée")
  MSEStaTrain <- window(MSESta, end=c(2015,4))
  MSEStaTest <- window(MSESta, start=2016)
  MSETrain <- window(MSE, end=c(2015,4))
  MSETest <- window(MSE, start=2016)
```

Nous nous intéressons aux ACF, PACF et test de KPSS afin de vérifier si les résidus obtenus à l'aide de la fonction decompose sont stationnaires. Bien que l'ACF et la PACF nous mettent en garde d'une possible non stationnarité de la série, la p-value du test de KPSS nous amène à conserver l'hypothèse de stationnarité de la série avec un seuil de confiance à 5%.

## PIB

```{r}
  PIBSta <- na.omit(decompose(PIB, "multiplicative")$random)
  par(mfrow=c(1,2))
  acf(PIBSta, main="Auto-Corrélation du PIB
      trimestrielle stationnarisée")
  pacf(PIBSta, main="Auto-Corrélation partielle du PIB
      trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(PIBSta)
  plot(PIBSta, main="PIB trimestriel stationnarisé")
  PIBStaTrain <- window(PIBSta, end=c(2015,4))
  PIBStaTest <- window(PIBSta, start=2016)
  PIBTrain <- window(PIB, end=c(2015,4))
  PIBTest <- window(PIB, start=2016)
```

Nous nous intéressons aux ACF, PACF et test de KPSS afin de vérifier si les résidus obtenus à l'aide de la fonction decompose sont stationnaires. Au regard de ces différentes informations, nous pouvons conclure à la stationnarité des résidus.

## SMIC

```{r}
  SMICSta <- na.omit(decompose(SMIC)$random)
  par(mfrow=c(1,2))
  acf(SMICSta, main="Auto-Corrélation du SMIC
      trimestrielle stationnarisée")
  pacf(SMICSta, main="Auto-Corrélation partielle du SMIC
      trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(SMICSta)
  plot(SMICSta, main="SMIC trimestriel stationnarisé")
  SMICStaTrain <- window(SMICSta, end=c(2015,4))
  SMICStaTest <- window(SMICSta, start=2016)
  SMICTrain <- window(SMIC, end=c(2015,4))
  SMICTest <- window(SMIC, start=2016)
```

Comme pour la masse salariale, les ACF et PACF semblent montrer que la série résiduelle pourrait ne pas être stationnaire. Cependant le test de KPSS nous permet de conserver l'hypothèse de stationnarité des résidus.

## Taux de chômage des femmes

```{r}
  TCHOFSta <- na.omit(decompose(TCHOF)$random)
  par(mfrow=c(1,2))
  acf(TCHOFSta, main="Auto-Corrélation du Taux de
      chômage des femmes
      trimestrielle stationnarisée")
  pacf(TCHOFSta, main="Auto-Corrélation partielle
      du Taux de chômage des femmes
      trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(TCHOFSta)
  plot(TCHOFSta, main="Taux de chômage trimestriel des femmes stationnarisé")
  TCHOFStaTrain <- window(TCHOFSta, end=c(2015,4))
  TCHOFStaTest <- window(TCHOFSta, start=2016)
  TCHOFTrain <- window(TCHOF, end=c(2015,4))
  TCHOFTest <- window(TCHOF, start=2016)
```

En ce qui concerne le taux de chômage des femmes, en regardant l'ACF, PACF et le test de KPSS, on peut conclure que la série résiduelle est stationnaire.

Maintenant que toutes les séries ont été stationnarisées, nous allons pouvoir construire des modèles VAR.

# Calcul de l'ordre p

Afin de mettre en place une modélisation VAR, nous devons dans un premier temps nous intéresser à l'ordre p du modèle VAR. L'ordre p correspond à l'ordre de l'opérateur de retard, c'est-à-dire le nombre de valeurs du passé qui ont un impact sur la valeur à un instant défini. Dans le package **MTS**, la fonction utilisée est VARorder, qui comme VARselect utilise les critères d'AIC, BIC et HQ afin de déterminer l'ordre du processus. Toutefois, le critère FPE n'est pas présent, ce qui nous conforte dans notre idée qu'il n'est pas très utile à notre étude. Les valeurs des différents indicateurs nous font retenir un ordre p=3.

```{r}
selec <- VARorder(cbind(MSEStaTrain, PIBStaTrain, SMICStaTrain, TCHOFStaTrain), maxp=10)
```


# Estimation du modèle

Dans le package **MTS**, la fonction utilisée pour construire des modèles VAR est *VAR,* qui prend en entrée la série temporelle multivariée et l'ordre du processus. On affiche ci-dessous les résultats renvoyés par la fonction sur le modèle
```{r}
modele<-VAR(cbind(MSEStaTrain, PIBStaTrain, SMICStaTrain, TCHOFStaTrain), p=3, output=F)
print("Coefficients du modèle")
coeff<-modele$coef[2:5,]
colnames(coeff)<-rownames(coeff)<-c("MSE", "PIB", "SMIC", "TCHOF")
coeff
print("Erreurs standard des coefficients")
se<-modele$secoef[2:5,]
colnames(se)<-rownames(se)<-c("MSE", "PIB", "SMIC", "TCHOF")
se
```

```{r}
matrix(data=c(modele$aic, modele$bic, modele$hq), nrow=1, byrow=T, dimnames=list(NULL, c("AIC", "BIC", "HQ")))
```
Afin de vérifier que les résidus du modèle ne présentent ni d'auto-corrélation (corrélation dans une série entre deux temps différents), ni de corrélation croisée (corrélation entre deux séries différentes).

```{r}
LjungBox(modele$residuals, 1:10)
```

L'hypothèse nulle d'indépendance des résidus est conservée, ce qui nous permet de valider notre modèle.

# Prévisions

Maintenant que nous avons estimé l'ordre des différents modèle VAR, et que nous avons explicité l'estimation des modèles, nous cherchons désormais à trouver celui dont les prédictions sont les plus proches de la réalité.

Après avoir comparé tous les modèles possibles (7 : 3 modèles avec deux variables, 3 modèles avec trois variables et un modèle avec les quatre variables), nous nous apercevons que le meilleur en terme de prédictions est le modèle prenant en compte le SMIC et le taux de chômage.

```{r, results="hide"}
#SMIC
VARorder(cbind(MSEStaTrain, SMICStaTrain, TCHOFStaTrain), maxp=10)
modele<-VAR(cbind(MSEStaTrain, SMICStaTrain, TCHOFStaTrain), p=3)
pred <- VARpred(modele, 4)
```

```{r}
plot(window(MSETest, end=c(2016,4)), main="Différences entre les véritables
     valeurs de 2016 et les prédictions du modèle pour la masse salariale")
#Reconstruction de la variable stationnaire
recon <- ts(pred$pred[,1], start=2016, frequency=4) * MSETrendTest * MSESeasonalTest
lines(recon, col = "red")
legend('bottomleft', legend = c('Série MSE', 'Prévisions du modèle'),
       col=c('black', 'red'), lty=1, cex=0.8)
```

Nous nous intéressons donc à l'erreur quadratique moyenne de cette prévision, qui est inférieure à celle obtenue pour le meilleur modèle effectué avec le package **vars**.

```{r}
EQM(MSETest, recon)
```