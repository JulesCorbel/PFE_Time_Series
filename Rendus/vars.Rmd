---
title: "Package vars"
author: "Paul GUILLOTTE & Jules CORBEL"
date: "12/02/2019"
output: 
  pdf_document:
    fig_caption: yes
---

```{r setup, include=FALSE}
require(tseries)
require(forecast)
require(corrplot)
require(fUnitRoots)
require(vars)
require(portes)

#Code permettant la mise en place de la représentation visuelle de la matrice des corrélations
cor.mtest <- function(mat, conf.level = 0.95){
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
  diag(p.mat) <- 0
  diag(lowCI.mat) <- diag(uppCI.mat) <- 1
  for(i in 1:(n-1)){
    for(j in (i+1):n){
      tmp <- cor.test(mat[,i], mat[,j], conf.level = conf.level)
      p.mat[i,j] <- p.mat[j,i] <- tmp$p.value
      lowCI.mat[i,j] <- lowCI.mat[j,i] <- tmp$conf.int[1]
      uppCI.mat[i,j] <- uppCI.mat[j,i] <- tmp$conf.int[2]
    }
  }
  return(list(p.mat, lowCI.mat, uppCI.mat))
}

EQM<-function(serie, prediction){
  return(sum((serie - prediction)^2)/length(serie))
}

trim <- read.csv("~/PFE_Time_Series/Data/Data_Trim.csv", sep=";", dec=",")
```

Nous nous intéresserons dans ce document à la mise en place de modèles VAR afin de prédire la masse salariale trimestrielle. Un modèle VAR, pour  Vecteur AutoRégressif, a pour objectif de capturer les interdépendances entre les différentes séries temporelles à notre disposition. Ainsi, chaque variable est expliquée par ses propres valeurs passées ainsi que par les valeurs passées des autres variables du modèle.

# Visualisation des séries

Nous nous intéressons dans cette partie aux différentes séries trimestrielles à notre disposition. Dans un premier temps, nous nous intéressons aux corrélations entre les variables deux à deux afin de nous faire une première idée du lien qu'il existe entre les variables.

##Masse salariale

```{r}
  MSE <- ts(trim$MSE, start = 1990, end = c(2017, 2), frequency=4)
  plot(MSE, main="Evolution trimestrielle de la masse salariale", xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
  par(mfrow=c(1,2))
  acf(MSE, main="Auto-corrélation de la
      masse salariale trimestrielle", lag.max=40)
  pacf(MSE, main="Autocorrélation partielle
       de la masse trimestrielle", lag.max=40)
  kpss.test(MSE)
  adf.test(MSE)
```

La masse salariale trimestrielle possède une composante de tendance de 1990 à 2010. La série tend par la suite à stagner. Nous remarquons également une saisonnalité sur cette série, qui est de plus en plus marquée à mesure que le temps passe. 

Comme la série comporte une tendance et une saisonnalité, elle ne correspond pas aux deux premières conditions de la stationnarité du second ordre, soit que la série possède une moyenne et un écart-type constants. Cela est confirmé par la fonction ACF qui décroît régulièrement. Nous effectuons également un test de KPSS (test de stationnarité) servant à vérifier si la série est stationnaire ou non (sous l'hypothèse $H_{0}$ la série est stationnaire, et sous l'hypothèse $H_{1}$ elle ne l'est pas). La série est dite stationnaire si ses propriétés statistiques (espérance, variance et auto-corrélation) sont fixes au cours du temps. La p-value est de 0.01 ce qui nous confirme que la série n'est pas stationnaire avec un risque de première espèce de 5%. Nous mettons également en place un test de racines unitaires, le test de Dickey Fuller augmenté. Son hypothèse nulle est que la série a été générée par un processus présentant une racine unitaire, et donc que la série n'est pas stationnaire. Ici, avec un risque de premier espèce à 5%, on conserve l'hypothèse nulle est on conclut, à l'aide des deux tests effectués, que la série n'est pas stationnaire. 

##PIB 

```{r}
  PIB <- ts(trim$PIB, start = 1990, end = c(2017, 1), frequency=4)
  plot(PIB, main="Evolution trimestrielle du PIB",xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
  par(mfrow=c(1,2))
  acf(PIB, main="Auto-corrélation 
      du PIB trimestriel", lag.max=40)
  pacf(PIB, main="Autocorrélation partielle
       du PIB trimestriel", lag.max=40)
  par(mfrow=c(1,1))
  kpss.test(PIB)
  adf.test(PIB)
```

Comme pour la masse salariale, le PIB annuel possède une tendance. Cependant, il ne semble pas posséder de saisonnalité. Cette série ne semble donc pas non plus stationnaire. Nous effectuons à nouveau un test de KPSS. La p-value est de 0.01 ce qui nous confirme que la série n'est pas stationnaire avec un risque de première espèce de 5%. Même conclusion au regard du test augmenté de Dickey Fuller.

##SMIC
```{r}
  SMIC <- ts(trim$SMIC, start = c(1990,1), end = c(2017, 4), frequency = 4)
  plot(SMIC, main="Evolution trimestrielle du SMIC", xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
  par(mfrow=c(1,2))
  acf(SMIC, main="Auto-corrélation du
      SMIC trimestriel", lag.max=20)
  pacf(SMIC, main="Autocorrélation partielle
       du SMIC trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(SMIC)
  adf.test(SMIC)
```

Au regard de la représentation graphique, on s'aperçoit qu'il y a bien une tendance. Pour la saisonnalité, il est plus difficile de savoir s'il en existe une ou pas, puisque la série semble augmenter seulement à certains temps. Les tests de KPSS et de Dickey Fuller augmenté nous confirment que la série n'est pas stationnaire.

##Taux de chômage des femmes

```{r}
  TCHOF <- ts(trim$TCHOF, start = c(1990,1), end = c(2017, 4), frequency = 4)
  plot(TCHOF, main="Evolution trimestrielle du taux de chômage des femmes", xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
  par(mfrow=c(1,2))
  acf(TCHOF, main="Auto-corrélation du taux de
      chômage des femmes trimestriel", lag.max=20)
  pacf(TCHOF, main="Autocorrélation partielle du
       taux de chômage des femmes trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(TCHOF)
  adf.test(TCHOF)
```

Pour cette dernière série qui représente le taux de chômage trimestriel des femmes, il ne semble pas y avoir de saisonnalité. On remarque cependant qu'il y a bien une tendance, au regard de la fonction d'auto-corrélation. En regardant la série de plus près, on s'aperçoit que la tendance semble être "par morceaux" : d'abord une hausse de 1990 à 1996, puis elle décroît jusqu'en 2002, avant d'augmenter à nouveau jusqu'en 2007, de chuter jusqu'en 2010. Si la série ne possède pas une tendance uniforme sur toute la durée étudiée, elle semble donc bien posséder une tendance par morceaux. Les tests KPSS et de Dickey Fuller augmenté nous confirment que la série n'est pas stationnaire, avec un risque de première espèce de 5%.

##Calcul des corrélations

```{r}
corrplot(cor(trim[1:109,-1]), method = "number", type="lower",
         p.mat=cor.mtest(trim[1:109,-1], 0.95)[[1]], insig="pch",
         col=colorRampPalette(c("blue", "light blue", "red"))(50), title = "
         Corrélations entre les variables trimestrielles")
corr <- cor.mtest(trim[1:109,-1], 0.95)[[1]]
rownames(corr) <- c("MSE","PIB","SMIC","TCHOF")
colnames(corr) <- c("MSE","PIB","SMIC","TCHOF")
corr
```

On se rend compte que le taux de chômage des femmes est corrélé négativement avec toutes les autres variables. Le trio de variables PIB, masse salariale et SMIC sont extrêmement liées entre elles. En regardant le tableau des p-values associées au test de Student (H0 : La corrélation entre les deux variables est nulle), on s'aperçoit que toutes les variables prises deux à deux présentes une corrélation.

# Transformation des séries

Pour chacune des séries, nous allons créé un échantillon d'apprentissage, qui nous permettra de construire les différents modèles, ainsi qu'un échantillon de test, qui nous permettra de comparer les prédictions des modèles construits avec des vraies valeurs. L'échantillon d'apprentissage sera composé de toutes les valeurs jusqu'au 4e trimestre 2015, et celui de test de toutes les valeurs à partir du 1er trimestre 2016. **Utiliser des (S)AR(I)MA pour estimer et stationnariser les modèles**

Nous allons maintenant transformer les séries pour les rendre stationnaires, afin de pouvoir appliquer les différents modèles ensuite. Afin de stationnariser les séries, nous utiliserons la fonction decompose qui permet de découper la série en trois : la tendance, la saisonnalité et les résidus, afin de pouvoir ensuite travailler avec les résidus. Nous ne stationnariserons que les échantillons d'apprentissage.

## Masse salariale

```{r}
  MSETrain <- window(MSE, end=c(2015,4))
  MSETest <- window(MSE, start=2016)
  plot(decompose(MSETrain, "multiplicative"))
  MSESta <- na.omit(decompose(MSETrain, "multiplicative")$random)
  #MSETrendTest<-window(decompose(MSETrain, "multiplicative")$trend)
  #MSESeasonalTest<-window(decompose(MSETrain, "multiplicative")$seasonal)
  plot(MSESta, main="Masse salariale trimestrielle stationnarisée", xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
  par(mfrow=c(1,2))
  acf(MSESta, main="Auto-Corrélation de la Masse
      salariale trimestrielle stationnarisée")
  pacf(MSESta, main="Auto-Corrélation partielle de la Masse
       salariale trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(MSESta)
  adf.test(MSESta)
```

Nous nous intéressons aux ACF, PACF et test de KPSS afin de vérifier si les résidus obtenus à l'aide de la fonction decompose sont stationnaires. Bien que l'ACF et la PACF nous mettent en garde d'une possible non stationnarité de la série, la p-value des tests de KPSS et Dickey Fuller augmenté nous amène à confirmer que notre série est désormais stationnarisée (avec un seuil de confiance à 5% pour les deux tests effectués).

## PIB

```{r}
  PIBTrain <- window(PIB, end=c(2015,4))
  PIBTest <- window(PIB, start=2016)
  PIBSta <- na.omit(decompose(PIBTrain, "multiplicative")$random)
  par(mfrow=c(1,2))
  plot(PIBSta, main="PIB trimestriel stationnarisé", xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
  acf(PIBSta, main="Auto-Corrélation du PIB
      trimestrielle stationnarisée")
  pacf(PIBSta, main="Auto-Corrélation partielle du PIB
      trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(PIBSta)
  adf.test(PIBSta)
```

Nous nous intéressons aux ACF, PACF, test de KPSS et test de Dickey Fuller augmenté afin de vérifier si les résidus obtenus à l'aide de la fonction decompose sont stationnaires. Au regard de ces différentes informations, nous pouvons conclure à la stationnarité des résidus.

## SMIC

```{r}
  SMICTrain <- window(SMIC, end=c(2015,4))
  SMICTest <- window(SMIC, start=2016)
  SMICSta <- na.omit(decompose(SMICTrain)$random)
  plot(SMICSta, main="SMIC trimestriel stationnarisé", xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
  par(mfrow=c(1,2))
  acf(SMICSta, main="Auto-Corrélation du SMIC
      trimestrielle stationnarisée")
  pacf(SMICSta, main="Auto-Corrélation partielle du SMIC
      trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(SMICSta)
  adf.test(SMICSta)
```

Comme pour la masse salariale, les ACF et PACF semblent montrer que la série résiduelle pourrait ne pas être stationnaire. Cependant le test de KPSS ainsi que le test de Dickey Fuller augmenté nous permettent de conclure à la stationnarité des résidus.

## Taux de chômage des femmes

```{r}
  TCHOFTrain <- window(TCHOF, end=c(2015,4))
  TCHOFTest <- window(TCHOF, start=2016)
  TCHOFSta <- na.omit(decompose(TCHOFTrain)$random)
  plot(TCHOFSta, main="Taux de chômage trimestriel des femmes stationnarisé", xaxt="n")
  axis(side=1, at=seq(1990,2015,5), labels=c("1990Q1", "1995Q1", "2000Q1", "2005Q1", "2010Q1", "2015Q1"))
  par(mfrow=c(1,2))
  acf(TCHOFSta, main="Auto-Corrélation du Taux de
      chômage des femmes
      trimestrielle stationnarisée")
  pacf(TCHOFSta, main="Auto-Corrélation partielle
      du Taux de chômage des femmes
      trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(TCHOFSta)
  adf.test(TCHOFSta)
```

En ce qui concerne le taux de chômage des femmes, en regardant l'ACF, PACF, le test de KPSS et le test de Dickey Fuller augmenté, on peut conclure que la série résiduelle est stationnaire.

Maintenant que toutes les séries ont été stationnarisées, nous allons pouvoir construire des modèles VAR.

##Corrélation entre les variables stationnarisées

```{r}
corrplot(cor(cbind(MSESta, PIBSta, SMICSta, TCHOFSta)), method = "number", type="lower",
         p.mat=cor.mtest(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), 0.95)[[1]], insig="n",
         col=colorRampPalette(c("blue", "light blue", "red"))(50), title = "
         Corrélations entre les variables trimestrielles")
corr <- cor.mtest(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), 0.95)[[1]]
rownames(corr) <- c("MSE","PIB","SMIC","TCHOF")
colnames(corr) <- c("MSE","PIB","SMIC","TCHOF")
corr
```

On s'aperçoit que la transformation de nos séries a permis de supprimer les corrélations entre elles. En effet, en regardant le tableau des p-value, l'hypothèse nulle d'absence de corrélations n'est rejeté pour aucun couple de varaiables (avec un seuil de 5%).

# Calcul de l'ordre p

Afin de mettre en place une modélisation VAR, nous devons dans un premier temps nous intéresser à l'ordre p du modèle VAR. L'ordre p correspond à l'ordre de l'opérateur de retard, c'est-à-dire le nombre de valeurs du passé qui ont un impact sur la valeur à un instant défini. Dans le package **vars**, la fonction VARselect permet de déterminer les l'ordre des modèles VAR à selectionner en fonction de 4 critères (AIC, HQ, SC et FPE).

Pour les critères suivants, p correspond à l'ordre du modèle VAR, T le nombre d'observations utilisées pour la phase d'apprentissage, K le nombre de variables et $\tilde{\Sigma}_u (p) = \frac{1}{T} \Sigma_{t=1}^T \hat{u}_t \hat{u}_t'$ (la matrice de covariance des résidus du modèle).

Le critère AIC (Aikaike information criterion) se calcule, dans ce package, de la manière suivante : $AIC(p) = \ln \det(\tilde{\Sigma}_u(p)) + \frac{2}{T}p K^2 \quad$. L'objectif est de minimiser ce critère. Cela suppose donc que le déterminant de la matrice $\tilde{\Sigma}_u(p)$ soit strictement positif. Ce critère est asymptotiquement effiace : si le nombre d'observations tend vers l'infini, sa variance est aussi faible que possible.

Le critère HQ (Hannan-Quinn criterion) se calcule, dans ce package, de la manière suivante : $HQ(p) = \ln \det(\tilde{\Sigma}_u(p)) + \frac{2 \ln(\ln(T))}{T}p K^2 \quad$. L'objectif est de minimiser ce critère. Encore une fois, cela suppose que le déterminant de la matrice $\tilde{\Sigma}_u(p)$ soit strictement positif.

Le critère SC (Schwarz criterion) se calcule dans ce package de la manière suivante : $SC(p) = \ln \det(\tilde{\Sigma}_u(p)) + \frac{\ln(T)}{T}p K^2 \quad$. L'objectif est de minimiser ce critère. Ce critère est équivalent au BIC.`

Dans cette partie, nous développerons le fonctionement de la méthodologie en l'appliquant uniquement au modèle complet, soit celui prenant en compte les variables PIB, SMIC et taux de chômage des femmes.

```{r}
selec <- VARselect(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), lag.max=10)
par(mfrow=c(2,2))
plot(seq(1:10),t(selec$criteria[1,]), type="l", main="Evolution de l'AIC en
     fonction de l'ordre",
     xlab="Ordre", ylab="AIC")
abline(v=which.min(selec$criteria[1,]), col="blue")
plot(seq(1:10),t(selec$criteria[2,]), type="l", main="Evolution du critère HQ
     en fonction de l'ordre",
     xlab="Ordre", ylab="HQ")
abline(v=which.min(selec$criteria[2,]), col="blue")
plot(seq(1:10),t(selec$criteria[3,]), type="l", main="Evolution du SC en
     fonction de l'ordre",
     xlab="Ordre", ylab="SC")
abline(v=which.min(selec$criteria[3,]), col="blue")
```

On s'aperçoit que les différents critères à notre disposition nous donnent des ordres à choisir différents. Ainsi, le meilleur AIC correspond à un modèle d'ordre 10, le meilleur HQ à un modèle d'ordre 3 et le meilleur SC à un modèle d'ordre 2. L'ordre de l'AIC étant trop grand (car trop de coefficients à estimer par rapport au nombre d'observations à notre disposition), nous ne souhaitons pas conserver cet ordre. De plus, on se rend compte que l'AIC du modèle avec un ordre 10 est similaire à celle d'un modèle avec un ordre 4. Les modèles HQ et SC sont meilleurs avec respectivement un ordre 3 et 2. Nous allons donc, dans la suite de l'analyse, essayer les trois modèles définit par les différents critères : ici, nous allons donc nous intéresser aux modèles d'ordre 2, 3 et 4. ** Tester tous les ordres définit par les 3 critères : Ici, on devra tester les modèles d'ordre 2, 3 et 4 **

# Estimation du modèle VAR(p)

Dans la partie précédente, nous avons sélectionné le meilleur ordre pour notre  modèle VAR. Il s'agit maintenant d'estimer différents modèles afin de pouvoir prédire la MSE. L'exemple que nous avons pris est pour le modèle complet, avec les trois ordres déterminés précédemment (2, 3 et 4). 

Un modèle VAR s'écrit sous la forme suivante : 

  $y_t = \sum_{i = 1}^{p} {A_iy_{t-i}} + u_t$

$A_i$ représentent les matrices de coefficients du modèle pour un ordre i, t le décalage de la série et $u_t$ une matrice K-dimensionnelle composée des résidus du modèle (indépendants et identiquement distribués).

##Ordre 4

Dans le package **vars**, la fonction utilisée pour construire des modèles VAR est VAR, qui prend en entrée la série temporelle multivariée, l'ordre du processus et le type de régresseurs à inclure. Dans notre cas, *type* vaut *const* car la série est stationnarisée et donc centrée en une constante $\mu$. Ci-dessous, le modèle d'ordre 4.

```{r, results="hide"}
modele<-VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=4, type="const")
```

Les coefficients du modèle associés à un retard de 1 sont les suivants :

```{r}
A1<-cbind(modele$varresult$MSESta$coefficients[1:4], 
          modele$varresult$PIBSta$coefficients[1:4], 
          modele$varresult$SMICSta$coefficients[1:4],
          modele$varresult$TCHOFSta$coefficients[1:4])
colnames(A1)<-rownames(A1)<-c("MSE", "PIB", "SMIC", "TCHOF")
A1
```

Dans cette fonction, nous ne disposons pas des erreurs standards associées aux coefficients. Les indicateurs de qualité du modèle sont présents ci-dessous.

```{r}
selection<-VARselect(cbind(MSESta, PIBSta, SMICSta, TCHOFSta))
selection$criteria[,4]
```

L'erreur quadratique moyenne de ce modèle pour les données prédites est la suivante :

```{r}
#ordre4 <- forecast(VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=4, type="const"))
#EQM(ordre4$forecast$MSESta$mean)
```

##Ordre 3

On s'intéresse ensuite au modèle d'ordre 3 :

```{r, results="hide"}
modele2<-VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=3, type="const")
```

Les coefficients du modèle associés à un retard de 1 sont les suivants :

```{r}
A1modele2<-cbind(modele2$varresult$MSESta$coefficients[1:4], 
          modele2$varresult$PIBSta$coefficients[1:4], 
          modele2$varresult$SMICSta$coefficients[1:4],
          modele2$varresult$TCHOFSta$coefficients[1:4])
colnames(A1modele2)<-rownames(A1modele2)<-c("MSE", "PIB", "SMIC", "TCHOF")
A1modele2
```

Les indicateurs de qualité du modèle sont présents ci-dessous.

```{r}
selection<-VARselect(cbind(MSESta, PIBSta, SMICSta, TCHOFSta))
selection$criteria[,3]
```

L'erreur quadratique moyenne de ce modèle pour les données prédites est la suivante :

```{r}
#ordre4 <- forecast(VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=3, type="const"))
#EQM(ordre4$forecast$MSESta$mean)
```

##Ordre 2

On s'intéresse ensuite au modèle d'ordre 3 :

```{r, results="hide"}
modele3<-VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=2, type="const")
```

Les coefficients du modèle associés à un retard de 1 sont les suivants :

```{r}
A1modele3<-cbind(modele3$varresult$MSESta$coefficients[1:4], 
          modele3$varresult$PIBSta$coefficients[1:4], 
          modele3$varresult$SMICSta$coefficients[1:4],
          modele3$varresult$TCHOFSta$coefficients[1:4])
colnames(A1modele3)<-rownames(A1modele3)<-c("MSE", "PIB", "SMIC", "TCHOF")
A1modele3
```

Les indicateurs de qualité du modèle sont présents ci-dessous.

```{r}
selection<-VARselect(cbind(MSESta, PIBSta, SMICSta, TCHOFSta))
selection$criteria[,2]
```

L'erreur quadratique moyenne de ce modèle pour les données prédites est la suivante :

```{r}
#ordre4 <- forecast(VAR(cbind(MSESta, PIBSta, SMICSta, TCHOFSta), p=2, type="const"))
#EQM(ordre4$forecast$MSESta$mean)
```

# Vérification des hypothèses

Afin que le modèle soit valide, il faut vérifier les trois hypothèses suivantes : homoscédasticité des résidus (test multivarié de ARCH-LM), normalité des résidus (test Jarque-Bera) et absence de corrélations entre les résidus (test du Portmanteau multivarié). Nous allons donc appliquer ces trois tests.

## Verification de la stabilité

Pour que le processus VAR soit stable, c'est-à-dire qu'il génère des séries stationnaires, nous devons calculer les valeurs propres de la matrice des coefficients  : 

$A $

et s'assurer que la solution de cette équation a des racines pour $z = 1$

```{r, include=FALSE}
A2<-cbind(modele$varresult$MSESta$coefficients[5:8], 
          modele$varresult$PIBSta$coefficients[5:8], 
          modele$varresult$SMICSta$coefficients[5:8],
          modele$varresult$TCHOFSta$coefficients[5:8])
colnames(A2)<-rownames(A2)<-c("MSE", "PIB", "SMIC", "TCHOF")
A2
A3<-cbind(modele$varresult$MSESta$coefficients[9:12], 
          modele$varresult$PIBSta$coefficients[9:12], 
          modele$varresult$SMICSta$coefficients[9:12],
          modele$varresult$TCHOFSta$coefficients[9:12])
colnames(A3)<-rownames(A3)<-c("MSE", "PIB", "SMIC", "TCHOF")
A3
A4<-cbind(modele$varresult$MSESta$coefficients[13:16],
          modele$varresult$PIBSta$coefficients[13:16],
          modele$varresult$SMICSta$coefficients[13:16],
          modele$varresult$TCHOFSta$coefficients[13:16])
colnames(A4)<-rownames(A4)<-c("MSE", "PIB", "SMIC", "TCHOF")
A4
```

Construction de la matrice A

```{r}
A <- matrix(0,nrow=16, ncol=16)
A[1:4,1:4] = A1
A[1:4,5:8] = A2
A[1:4,9:12] = A3
A[1:4,13:16] = A4
A[5:8,1:4] = diag(1,4,4)
A[9:12,5:8] = diag(1,4,4)
A[13:16,9:12] = diag(1,4,4)
vp <- eigen(A)
Mod(vp$values)
plot(seq(1,16), Mod(vp$values), xlab="Valeurs propres", ylab="Modules", ylim=c(0,1.5))
abline(h=1, col="red")
```

## Test ARCH (homoscédasticité des résidus)

Le test multivarié de ARCH-LM permet de tester l'homoscédasticité des résidus. La statistique de test est la suivante : $VARCH_{LM}(q) = \frac{1}{2}TK(K+1)R_m^2$, où $R_m^2 = 1 - \frac{2}{K(K+1)}tr(\hat{\Omega}\hat{\Omega}_O^{-1})$, et $\hat{\Omega}$ est la matrice de covariance de la régression suivante : $vech(\hat{u_t}\hat{u_t}^T) = \beta_0 + B_1 vech(\hat{u}_{t-1}\hat{u}_{t-1}^T) + ... + B_q vech(\hat{u}_{t-q}\hat{u}_{t-q}^T) + v_t$. La dimension de $\beta_O$ est $\frac{1}{2}K(K+1)$ et celle des matrices des coefficients $B_i$ est $\frac{1}{2}K(K+1) × \frac{1}{2}K(K+1)$. La statistique de test suit une loi de $\chi^2(qK^2(K+1)^2/4)$, donc dans notre cas $\chi^2(16q*25/4)$. L'hypothèse nulle de ce test est $H_0 : B_0 = B_1 = ... = B_q = 0$ (homoscédasticité).

```{r, fig.cap="\\label{fig1}"}
a1 <- c()
for(i in 1:18){
a1[i] <- arch.test(modele, lags.multi = i)$arch.mul$p.value
}
plot(a1, type="l", main="Evolution de la p-value en fonction du lag", xlab="lag", ylab="p-value")
abline(h=0.05, col="red")
```

On s'aperçoit au regard de la figure \ref{fig1}, avec un seuil de confiance de 5% (ligne rouge), qu'on rejette l'hypothèse nulle d'homoscédasticité pour un retard faible (inférieur à 7). Cependant, en augmentant le nombre de valeurs prises en compte pour calculer la nouvelle, on se rend compte qu'on conserve l'hypothèse d'homoscédasticité. On observe également que la valeur de la p-value converge vers 1 au fur et à mesure qu'on augmente le retard. Ainsi, en prenant l'ensemble des résidus, nous conservons l'hypothèse d'homoscédasticité.

## Test normalité (normalité des résidus)

Le test de Jarque-Bera pour séries multivariées permet de tester la normalité des résidus. Il utilise les résidus standardisés à l'aide d'une décomposition de Cholesky de la matrice de variance-covariance des résidus centrés. Il est important noter que l'ordre dans lequel les variables sont stockées dans la matrice a une importance sur les résultats. La statistique de test est la suivante : $JB_{mv} = s_3^2 + s_4^2$, où $s_3^2$ et $s_4^2$ se calculent de la sorte : $s_3^2 = Tb_1^Tb_1/6$ et $s_4^2 = T(b_2 - 3_K)^T(b_2-3_K)/24$, avec $b_1$ et  $b_2$ qui sont respectivement les vecteurs des moments non-centrés d'ordre trois et quatre des résidus standardisés. La statistique de test suit une loi de $\chi^2(2K)$. Ce test compare en fait le coefficient kurtosis K (l'aplatissement de la fonction de densité) et le coefficient skewness S (asymétrie de la fonction de densité) d'une loi normale à ceux des résidus testés. L'hypothèse nulle est donc $H0 : S = 0$ et $K = 3$.

```{r}
normality.test(modele)$jb.mul$JB
```

Ici, on rejette l'hypothèse H0, avec un seuil de confiance de 5%. Les résidus obtenus ne suivent pas une loi normale.

## Test Portmanteau (corrélations des résidus)

Le test de Portmanteau multivarié permet de tester l'auto-corrélation (au sein d'une même série) et la corrélation croisée (entre les différentes séries) des résidus.

### Verification de la Matrice $C_0$

La statistique de Portmanteau est $Q_h = T \sum_{j=1}^h tr(\hat{C}_j^T \hat{C}_0^{-1} \hat{C}_j \hat{C}_0^{-1})$, et elle suit une loi de $\chi^2(K^2h - n^*)$, $n^*$ étant le nombre de coefficients à estimer. Pour qu'elle existe, il faut donc vérifier que la matrice $\hat{C}_0$ est inversible pour que la statistique puisse être définie. Les matrices $\hat{C}_i$ s'écrivent $\hat{C}_i = \frac{1}{T} \sum_{t=i+1}^T \hat{u}_t \hat{u}_{t-i}^T$, donc $\hat{C}_0$ s'écrit $\hat{C}_0 = \frac{1}{T} \sum_{t=1}^T \hat{u}_t \hat{u}_t^T$. Nous allons donc vérifier qu'elle est inversible pour le modèle complet que nous avons mis en place. 

```{r}
C0 <- matrix(nrow = 4, ncol=4, 0)
for(i in 1:nrow(residuals(modele))){
  C0 <- C0 + residuals(modele)[i,]%*%t(residuals(modele)[i,])
}
C0 <- (1/nrow(residuals(modele))) * C0
C0
d <- det(C0)
names(d) <- "Déterminant de la matrice"
d
```

Le déterminant de la matrice n'étant pas nul, la matrice $\hat{C}_0$ est donc inversible.

### Application du test

Les 3 premières p-values ne peuvent être calculées à cause de la valeur des degrés de liberté. En effet, comme nous l'avons expliqué plus haut, la statistique de test suit une loi de $\chi^2(K^2h - n^*)$. Or, avec un retard compris entre 1 et 3, les degrés de liberté sont négatifs et il n'est donc pas possible d'appliquer le test. L'hypothèse nulle de ce test est l'absence de corrélations croisées et d'auto-corrélations.

```{r, fig.cap="\\label{fig2}"}
a1 <- c()
for(i in 1:3){
  a1[i] <- NA
} 
for(i in 4:50){
  a1[i] <- serial.test(modele, lags.pt=i, type="PT.asymptotic")$serial$p.value
}
plot(a1, type="l", main="Evolution de la p-value en fonction du lag", xlab="lag", ylab="p-value")
abline(h=0.05, col="red")
```

Au regarde la figure \ref{fig2} Comme pour le test ARCH, on rejette l'hypothèse nulle, avec un seuil de confiance à 5% (ligne rouge) pour un retard faible (5 ou moins). Cependant, pour un retard grand (supérieur à 5), on conserve l'hypothèse nulle d'absence d'auto-corrélations et de corrélations croisées. On observe également que la p-value converge vers 1 à mesure qu'on augmente le retard. Ainsi, en prenant en compte l'ensemble des résidus, on conserve l'hypothèse d'absence d'auto-corrélations et de corrélations croisées.

# Prévisions

Maintenant que nous avons estimé l'ordre des différents modèle VAR, et que nous avons explicité l'estimation des modèles, nous cherchons désormais à trouver celui dont les prédictions sont les plus proches de la réalité.

Après avoir comparé tous les modèles possibles (7 : 3 modèles avec deux variables, 3 modèles avec trois variables et un modèle avec les quatre variables), nous nous apercevons que le meilleur en terme de prédictions est le modèle prenant en compte le SMIC (en plus de la masse salariale).

```{r}
#SMIC
VARselect(cbind(MSESta, SMICSta), lag.max=10)
VAR(cbind(MSESta, SMICSta), p=3, type="const")
VARSMICSta <- forecast(VAR(cbind(MSESta, SMICSta), p=3, type="const"))
plot(MSETest, xlim=c(2016,2017.25), main="Différences entre les véritables
     valeurs de 2016 et les prédictions du modèle pour la masse salariale", xaxt="n")
axis(side=1, at=seq(2016,2017.25,0.25), labels=c("2016Q1", "2016Q2", "2016Q3", "2016Q4", "2017Q1", "2017Q2"))
#Reconstruction de la variable stationnaire
#recon <- VARSMICSta$forecast$MSESta$mean * MSETrendTest * MSESeasonalTest
#lines(recon, col = "red")
#legend('bottomleft', legend = c('Vraies valeurs', 'Prévisions du modèle'),
#       col=c('black', 'red'), lty=1, cex=0.8)
```

Nous nous intéressons donc à l'erreur quadratique moyenne de cette prévision.

```{r}
#EQM(MSETest, recon)
```