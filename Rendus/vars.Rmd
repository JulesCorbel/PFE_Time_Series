---
title: "Package vars"
author: "Paul GUILLOTTE & Jules CORBEL"
date: "01/02/2019"
output: pdf_document
---

```{r setup, include=FALSE}
require(tseries)
require(forecast)
require(corrplot)
require(fUnitRoots)
require(vars)
require(portes)

#Code permettant la mise en place de la représentation visuelle de la matrice des corrélations
cor.mtest <- function(mat, conf.level = 0.95){
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
  diag(p.mat) <- 0
  diag(lowCI.mat) <- diag(uppCI.mat) <- 1
  for(i in 1:(n-1)){
    for(j in (i+1):n){
      tmp <- cor.test(mat[,i], mat[,j], conf.level = conf.level)
      p.mat[i,j] <- p.mat[j,i] <- tmp$p.value
      lowCI.mat[i,j] <- lowCI.mat[j,i] <- tmp$conf.int[1]
      uppCI.mat[i,j] <- uppCI.mat[j,i] <- tmp$conf.int[2]
    }
  }
  return(list(p.mat, lowCI.mat, uppCI.mat))
}

EQM<-function(serie, prediction){
  return(sum((serie - prediction)^2)/length(serie))
}
```

Nous nous intéresserons dans ce document à la mise en place de modèles VAR afin de prédire la masse salariale trimestrielle. Un modèle VAR, pour  Vecteur AutoRégressif, a pour objectif de capturer les interdépendances entre les différentes séries temporelles à notre disposition. Ainsi, chaque variable est expliquée par ses propres valeurs passées ainsi que par les valeurs passées des autres variables du modèle.

# Visualation des séries

Nous nous intéressons dans cette partie aux différentes séries trimestrielles à notre disposition. Dans un premier temps, nous nous intéressons aux corrélations entre les variables deux à deux afin de nous faire une première idée du lien qu'il existe entre les variables.

```{r}
trim <- read.csv("~/PFE_Time_Series/Data/Data_Trim.csv", sep=";", dec=",")
corrplot(cor(trim[1:109,-1]), method = "number", type="lower",
         p.mat=cor.mtest(trim[1:109,-1], 0.95)[[1]], insig="pch",
         col=colorRampPalette(c("blue", "light blue", "red"))(50), title = "
         Corrélations entre les variables trimestrielles")
```

On se rend compte que le taux de chômage des femmes est corrélé négativement avec toutes les autres variables. Le trio de variables PIB, masse salariale et SMIC sont extrêmement liées entre elles.

Nous allons maintenant nous attarder sur chaque série individuellement.

##Masse salariale

```{r}
  MSE <- ts(trim$MSE, start = 1990, end = c(2017, 2), frequency=4)
  plot(MSE, main="Evolution de la masse salariale trimestrielle")
  par(mfrow=c(1,2))
  acf(MSE, main="Auto-corrélation de la
      masse salariale trimestrielle", lag.max=20)
  pacf(MSE, main="Autocorrélation partielle
       de la masse trimestrielle", lag.max=20)
  kpss.test(MSE)
```

La masse salariale trimestrielle possède une composante de tendance de 1990 à 2010. La série tend par la suite à stagner. Nous remarquons également une saisonnalité sur cette série, qui est de plus en plus marquée à mesure que le temps passe. 

Comme la série comporte une tendance et une saisonnalité, elle ne correspond pas aux deux premières conditions de la stationnarité du second ordre, soit que la série possède une moyenne et un écart-type constants. Cela est confirmé par la fonction ACF qui décroît régulièrement. Nous effectuons également un test de KPSS servant à vérifier si la série est stationnaire ou non (sous l'hypothèse $H_{0}$ la série est stationnaire, et sous l'hypothèse $H_{1}$ elle ne l'est pas). La p-value est de 0.01 ce qui nous confirme que la série n'est pas stationnaire avec un risque de première espèce de 5%.

##PIB 

```{r}
  PIB <- ts(trim$PIB, start = 1990, end = c(2017, 1), frequency=4)
  plot(PIB, main="Evolution du PIB trimestriel")
  par(mfrow=c(1,2))
  acf(PIB, main="Auto-corrélation 
      du PIB trimestriel", lag.max=20)
  pacf(PIB, main="Autocorrélation partielle
       du PIB trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(PIB)
```

Comme pour la masse salariale, le PIB annuel possède une tendance. Cependant, il ne semble pas posséder de saisonnalité. Cette série n'est donc pas non plus stationnaire. Nous effectuons à nouveau un test de KPSS. La p-value est de 0.01 ce qui nous confirme que la série n'est pas stationnaire avec un risque de première espèce de 5%.

##SMIC
```{r}
  SMIC <- ts(trim$SMIC, start = c(1990,1), end = c(2017, 4), frequency = 4)
  plot(SMIC, main="Evolution du SMIC trimestriel")
  par(mfrow=c(1,2))
  acf(SMIC, main="Auto-corrélation du
      SMIC trimestriel", lag.max=20)
  pacf(SMIC, main="Autocorrélation partielle
       du SMIC trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(SMIC)
```

Au regard de la représentation graphique, on s'aperçoit qu'il y a bien une tendance. Pour la saisonnalité, il est plus difficile de savoir s'il en existe une ou pas, puisque la série semble augmenter seulement à certains temps.

##Taux de chômage des femmes

```{r}
  TCHOF <- ts(trim$TCHOF, start = c(1990,1), end = c(2017, 4), frequency = 4)
  plot(TCHOF, main="Evolution du taux de chômage des femmes trimestriel")
  par(mfrow=c(1,2))
  acf(TCHOF, main="Auto-corrélation du taux de
      chômage des femmes trimestriel", lag.max=20)
  pacf(TCHOF, main="Autocorrélation partielle du
       taux de chômage des femmes trimestriel", lag.max=20)
  par(mfrow=c(1,1))
  kpss.test(TCHOF)
```

Pour cette dernière série qui représente le taux de chômage trimestriel des femmes, il ne semble pas y avoir de saisonnalité. On remarque cependant qu'il y a bien une tendance. Le test KPSS nous confirme que la série n'est pas stationnaire.

# Transformation des séries

Nous allons maintenant transformer les séries pour les rendre stationnaires, afin de pouvoir appliquer les différents modèles ensuite. Afin de stationnariser les séries, nous utiliserons la fonction decompose qui permet de découper la série en trois : la tendance, la saisonnalité et les résidus, afin de pouvoir ensuite travailler avec les résidus.

Pour chacune de ces séries, nous allons créé un échantillon d'apprentissage, qui nous permettra de construire les différents modèles, ainsi qu'un échantillon de test, qui nous permettra de comparer les prédictions des modèles construits avec des vraies valeurs. L'échantillon d'apprentissage sera composé de toutes les valeurs jusqu'au 4e trimestre 2015, et celui de test de toutes les valeurs à partir du 1er trimestre 2016.

## Masse salariale

```{r}
  plot(decompose(MSE, "multiplicative"))
  MSESta <- na.omit(decompose(MSE, "multiplicative")$random)
  MSETrendTest<-window(decompose(MSE, "multiplicative")$trend, start=2016, end=c(2016,4))
  MSESeasonalTest<-window(decompose(MSE, "multiplicative")$seasonal, start=2016, end=c(2016,4))
  par(mfrow=c(1,2))
  acf(MSESta, main="Auto-Corrélation de la Masse
      salariale trimestrielle stationnarisée")
  pacf(MSESta, main="Auto-Corrélation partielle de la Masse
       salariale trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(MSESta)
  plot(MSESta, main="Masse salariale trimestrielle stationnarisée")
  MSEStaTrain <- window(MSESta, end=c(2015,4))
  MSEStaTest <- window(MSESta, start=2016)
  MSETrain <- window(MSE, end=c(2015,4))
  MSETest <- window(MSE, start=2016)
```

Nous nous intéressons aux ACF, PACF et test de KPSS afin de vérifier si les résidus obtenus à l'aide de la fonction decompose sont stationnaires. Bien que l'ACF et la PACF nous mettent en garde d'une possible non stationnarité de la série, la p-value du test de KPSS nous amène à conserver l'hypothèse de stationnarité de la série avec un seuil de confiance à 5%.

## PIB

```{r}
  PIBSta <- na.omit(decompose(PIB, "multiplicative")$random)
  par(mfrow=c(1,2))
  acf(PIBSta, main="Auto-Corrélation du PIB
      trimestrielle stationnarisée")
  pacf(PIBSta, main="Auto-Corrélation partielle du PIB
      trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(PIBSta)
  plot(PIBSta, main="PIB trimestriel stationnarisé")
  PIBStaTrain <- window(PIBSta, end=c(2015,4))
  PIBStaTest <- window(PIBSta, start=2016)
  PIBTrain <- window(PIB, end=c(2015,4))
  PIBTest <- window(PIB, start=2016)
```

Nous nous intéressons aux ACF, PACF et test de KPSS afin de vérifier si les résidus obtenus à l'aide de la fonction decompose sont stationnaires. Au regard de ces différentes informations, nous pouvons conclure à la stationnarité des résidus.

## SMIC

```{r}
  SMICSta <- na.omit(decompose(SMIC)$random)
  par(mfrow=c(1,2))
  acf(SMICSta, main="Auto-Corrélation du SMIC
      trimestrielle stationnarisée")
  pacf(SMICSta, main="Auto-Corrélation partielle du SMIC
      trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(SMICSta)
  plot(SMICSta, main="SMIC trimestriel stationnarisé")
  SMICStaTrain <- window(SMICSta, end=c(2015,4))
  SMICStaTest <- window(SMICSta, start=2016)
  SMICTrain <- window(SMIC, end=c(2015,4))
  SMICTest <- window(SMIC, start=2016)
```

Comme pour la masse salariale, les ACF et PACF semblent montrer que la série résiduelle pourrait ne pas être stationnaire. Cependant le test de KPSS nous permet de conserver l'hypothèse de stationnarité des résidus.

## Taux de chômage des femmes

```{r}
  TCHOFSta <- na.omit(decompose(TCHOF)$random)
  par(mfrow=c(1,2))
  acf(TCHOFSta, main="Auto-Corrélation du Taux de
      chômage des femmes
      trimestrielle stationnarisée")
  pacf(TCHOFSta, main="Auto-Corrélation partielle
      du Taux de chômage des femmes
      trimestrielle stationnarisée")
  par(mfrow=c(1,1))
  kpss.test(TCHOFSta)
  plot(TCHOFSta, main="Taux de chômage trimestriel des femmes stationnarisé")
  TCHOFStaTrain <- window(TCHOFSta, end=c(2015,4))
  TCHOFStaTest <- window(TCHOFSta, start=2016)
  TCHOFTrain <- window(TCHOF, end=c(2015,4))
  TCHOFTest <- window(TCHOF, start=2016)
```

En ce qui concerne le taux de chômage des femmes, en regardant l'ACF, PACF et le test de KPSS, on peut conclure que la série résiduelle est stationnaire.

Maintenant que toutes les séries ont été stationnarisées, nous allons pouvoir construire des modèles VAR.

# Calcul de l'ordre p

Afin de mettre en place une modélisation VAR, nous devons dans un premier temps nous intéresser à l'ordre p du modèle VAR. L'ordre p correspond à l'ordre de l'opérateur de retard, c'est-à-dire le nombre de valeurs du passé qui ont un impact sur la valeur à un instant défini. Dans le package **vars**, la fonction VARselect permet de déterminer les valeurs de 4 critères (AIC, HQ, SC et FPE) pour différentes valeurs de l'ordre p.

Pour les critères suivants, n correspond à l'ordre de différenciation, T le nombre d'observations et K le nombre de variables et $\tilde{\Sigma}_u (n) = T^{-1} \Sigma_{t=1}^T \hat{u}_t \hat{u}_t'$.

Le critère AIC (Aikaike information criterion) se calcule, dans ce package, de la manière suivante : $AIC(n) = \ln \det(\tilde{\Sigma}_u(n)) + \frac{2}{T}n K^2 \quad$. L'objectif est de minimiser ce critère.

Le critère HQ (Hannan-Quinn criterion) se calcule, dans ce package, de la manière suivante : $HQ(n) = \ln \det(\tilde{\Sigma}_u(n)) + \frac{2 \ln(\ln(T))}{T}n K^2 \quad$. L'objectif est de minimiser ce critère. Contrairement à l'AIC, ce critère n'est pas asymptotiquement efficace.

Le critère SC (Schwarz criterion) se calcule dans ce package de la manière suivante : $SC(n) = \ln \det(\tilde{\Sigma}_u(n)) + \frac{\ln(T)}{T}n K^2 \quad$. L'objectif est de minimiser ce critère. Ce critère est équivalent au BIC.`

Le dernier critère, le critère FPE (Final Prediction Error), est un critère à minimiser. Cependant nous ne comprenons pas son fonctionnement. Nous ne l'utiliserons donc pas.

Dans cette partie, nous développerons le fonctionement de la méthodologie en l'appliquant uniquement au modèle complet, soit celui prenant en compte les variables PIB, SMIC et taux de chômage des femmes.

```{r}
selec <- VARselect(cbind(MSEStaTrain, PIBStaTrain, SMICStaTrain, TCHOFStaTrain), lag.max=10)
par(mfrow=c(2,2))
plot(seq(1:10),t(selec$criteria[1,]), type="l", main="Evolution de l'AIC en
     fonction de l'ordre",
     xlab="Ordre", ylab="AIC")
abline(v=which.min(selec$criteria[1,]), col="blue")
plot(seq(1:10),t(selec$criteria[2,]), type="l", main="Evolution du critère HQ
     en fonction de l'ordre",
     xlab="Ordre", ylab="HQ")
abline(v=which.min(selec$criteria[2,]), col="blue")
plot(seq(1:10),t(selec$criteria[3,]), type="l", main="Evolution du SC en
     fonction de l'ordre",
     xlab="Ordre", ylab="SC")
abline(v=which.min(selec$criteria[3,]), col="blue")
plot(seq(1:10),t(selec$criteria[4,]), type="l", main="Evolution du FPE en
     fonction de l'ordre",
     xlab="Ordre", ylab="FPE")
abline(v=which.min(selec$criteria[4,]), col="blue")
```

On s'aperçoit que les différents critères à notre disposition nous donnent des ordres à choisir différents. Ainsi, le meilleur AIC correspond à un modèle d'ordre 10, le meilleur HQ à un modèle d'ordre 3 et le meilleur SC à un modèle d'ordre 2. L'ordre de l'AIC étant trop grand (car trop de coefficients à estimer par rapport au nombre d'observations à notre disposition), nous ne souhaitons pas conserver cet ordre. De plus, on se rend compte que l'AIC du modèle avec un ordre 10 est similaire à celle d'un modèle avec un ordre 4. Les modèles HQ et SC sont meilleurs avec respectivement un ordre 3 et 2. Nous allons donc, dans la suite de l'analyse, veillez à minimiser le critère AIC, tout en étant attentif aux autres critères. C'est pourquoi, ici, nous décidons de conserver un ordre 4.

# Estimation du modèle

Dans la partie précédente, nous avons sélectionné le meilleur ordre pour notre  modèle VAR. Il s'agit maintenant d'estimer différents modèles afin de pouvoir prédire la MSE. L'exemple que nous avons pris est pour le modèle complet, qui nous donne donc un ordre de 4. 

Un modèle VAR s'écrit sous la forme suivante.: 

  $y_t = \sum_{i = 1}^{p} {A_iy_{t-i}} + u_t$

$A_i$ représentent les matrices de coefficients du modèle pour un ordre i, t le décalage de la série et $u_t$ le vecteur des résidus. Dans notre cas, nous aurons donc 3 matrices de taille 5x5. 

Dans le package **vars**, la fonction utilisée pour construire des modèles VAR est VAR, qui prend en entrée la série temporelle multivariée, l'ordre du processus et le type de régresseurs à inclure. Dans notre cas, *type* vaut *const* car la série est stationnarisée et donc centrée en une constante $\mu$.
```{r, results="hide"}
modele<-VAR(cbind(MSEStaTrain, PIBStaTrain, SMICStaTrain, TCHOFStaTrain), p=4, type="const")
```

Les coefficients du modèle sont les suivants
```{r}
A1<-cbind(modele$varresult$MSEStaTrain$coefficients[1:4], 
          modele$varresult$PIBStaTrain$coefficients[1:4], 
          modele$varresult$SMICStaTrain$coefficients[1:4],
          modele$varresult$TCHOFStaTrain$coefficients[1:4])
colnames(A1)<-rownames(A1)<-c("MSE", "PIB", "SMIC", "TCHOF")
A1
```

Dans cette fonction, nous ne disposons pas des erreurs standards associées aux coefficients. Les indicateurs de qualité du modèle sont présents ci-dessous.
```{r}
selection<-VARselect(cbind(MSEStaTrain, PIBStaTrain, SMICStaTrain, TCHOFStaTrain))
selection$criteria[,4]
```

# Prévisions

Maintenant que nous avons estimé l'ordre des différents modèle VAR, et que nous avons explicité l'estimation des modèles, nous cherchons désormais à trouver celui dont les prédictions sont les plus proches de la réalité.

Après avoir comparé tous les modèles possibles (7 : 3 modèles avec deux variables, 3 modèles avec trois variables et un modèle avec les quatre variables), nous nous apercevons que le meilleur en terme de prédictions est le modèle prenant en compte le SMIC (en plus de la masse salariale).

```{r}
#SMIC
VARselect(cbind(MSEStaTrain, SMICStaTrain), lag.max=10)
VAR(cbind(MSEStaTrain, SMICStaTrain), p=3, type="const")
VARSMICSta <- forecast(VAR(cbind(MSEStaTrain, SMICStaTrain), p=3, type="const"))
plot(MSETest, xlim=c(2016,2016.75), main="Différences entre les véritables
     valeurs de 2016 et les prédictions du modèle pour la masse salariale")
#Reconstruction de la variable stationnaire
recon <- VARSMICSta$forecast$MSEStaTrain$mean * MSETrendTest * MSESeasonalTest
lines(recon, col = "red")
legend('bottomleft', legend = c('Vraies valeurs', 'Prévisions du modèle'),
       col=c('black', 'red'), lty=1, cex=0.8)
```

Nous nous intéressons donc à l'erreur quadratique moyenne de cette prévision.

```{r}
EQM(MSETest, recon)
```